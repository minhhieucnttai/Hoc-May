# H∆∞·ªõng D·∫´n Chi Ti·∫øt T·ª´ng B∆∞·ªõc / Step-by-Step Tutorial

## B√†i 1: B·∫Øt ƒê·∫ßu V·ªõi D·ªØ Li·ªáu / Lesson 1: Getting Started with Data

### M·ª•c Ti√™u / Objectives:
- Hi·ªÉu c·∫•u tr√∫c d·ªØ li·ªáu / Understand data structure
- T·∫£i v√† kh√°m ph√° d·ªØ li·ªáu / Load and explore data
- X·ª≠ l√Ω d·ªØ li·ªáu c∆° b·∫£n / Basic data processing

### B∆∞·ªõc 1.1: T·∫£i D·ªØ Li·ªáu / Load Data

```python
import sys
sys.path.append('src')

from preprocessing import load_data

# T·∫£i d·ªØ li·ªáu / Load data
df = load_data('data/global_disaster_response_2018_2024.csv')

# Xem th√¥ng tin c∆° b·∫£n / View basic info
print(f"S·ªë d√≤ng / Rows: {len(df)}")
print(f"S·ªë c·ªôt / Columns: {len(df.columns)}")
print("\nT√™n c√°c c·ªôt / Column names:")
print(df.columns.tolist())
```

**Output mong ƒë·ª£i / Expected output:**
```
Data loaded successfully: 35 rows, 9 columns
S·ªë d√≤ng / Rows: 35
S·ªë c·ªôt / Columns: 9

T√™n c√°c c·ªôt / Column names:
['date', 'disaster_type', 'country', 'region', 'affected_population', 
 'casualties', 'response_time_hours', 'response_effectiveness', 'economic_impact_usd']
```

### B∆∞·ªõc 1.2: Kh√°m Ph√° D·ªØ Li·ªáu / Explore Data

```python
# Xem 5 d√≤ng ƒë·∫ßu / View first 5 rows
print("\n5 d√≤ng ƒë·∫ßu ti√™n / First 5 rows:")
print(df.head())

# Th·ªëng k√™ m√¥ t·∫£ / Descriptive statistics
print("\nTh·ªëng k√™ / Statistics:")
print(df.describe())

# Ki·ªÉm tra gi√° tr·ªã thi·∫øu / Check missing values
print("\nGi√° tr·ªã thi·∫øu / Missing values:")
print(df.isnull().sum())

# C√°c lo·∫°i thi√™n tai / Disaster types
print("\nC√°c lo·∫°i thi√™n tai / Disaster types:")
print(df['disaster_type'].value_counts())
```

### B∆∞·ªõc 1.3: Ti·ªÅn X·ª≠ L√Ω / Preprocessing

```python
from preprocessing import preprocess_data

# Ti·ªÅn x·ª≠ l√Ω ƒë·∫ßy ƒë·ªß / Full preprocessing
df_clean = preprocess_data('data/global_disaster_response_2018_2024.csv', save_output=False)

# So s√°nh tr∆∞·ªõc v√† sau / Compare before and after
print(f"\nTr∆∞·ªõc x·ª≠ l√Ω / Before: {len(df)} rows")
print(f"Sau x·ª≠ l√Ω / After: {len(df_clean)} rows")
```

---

## B√†i 2: K·ªπ Thu·∫≠t ƒê·∫∑c Tr∆∞ng / Lesson 2: Feature Engineering

### M·ª•c Ti√™u / Objectives:
- T·∫°o ƒë·∫∑c tr∆∞ng t·ª´ d·ªØ li·ªáu g·ªëc / Create features from raw data
- Hi·ªÉu c√°c lo·∫°i ƒë·∫∑c tr∆∞ng / Understand feature types
- M√£ h√≥a d·ªØ li·ªáu ph√¢n lo·∫°i / Encode categorical data

### B∆∞·ªõc 2.1: T·∫°o ƒê·∫∑c Tr∆∞ng Th·ªùi Gian / Create Temporal Features

```python
from feature_engineering import create_temporal_features

# T·∫°o ƒë·∫∑c tr∆∞ng th·ªùi gian / Create temporal features
df_temporal = create_temporal_features(df_clean)

# Xem c√°c c·ªôt m·ªõi / View new columns
new_cols = ['year', 'month', 'quarter', 'day_of_year', 'season']
print("\nƒê·∫∑c tr∆∞ng th·ªùi gian m·ªõi / New temporal features:")
print(df_temporal[new_cols].head())

# Ph√¢n t√≠ch theo m√πa / Analyze by season
print("\nThi√™n tai theo m√πa / Disasters by season:")
print(df_temporal['season'].value_counts())
```

### B∆∞·ªõc 2.2: T·∫°o Ch·ªâ S·ªë M·ª©c ƒê·ªô / Create Severity Index

```python
from feature_engineering import create_severity_index

# T·∫°o severity index / Create severity index
df_severity = create_severity_index(df_temporal)

# Xem ph√¢n ph·ªëi / View distribution
print("\nPh√¢n ph·ªëi Severity Index / Severity Index distribution:")
print(df_severity['severity_index'].describe())

# Top 5 thi√™n tai nghi√™m tr·ªçng nh·∫•t / Top 5 most severe disasters
print("\nTop 5 thi√™n tai nghi√™m tr·ªçng nh·∫•t / Top 5 most severe disasters:")
top5 = df_severity.nlargest(5, 'severity_index')[['date', 'disaster_type', 'country', 'severity_index']]
print(top5)
```

### B∆∞·ªõc 2.3: T·∫°o T·∫•t C·∫£ ƒê·∫∑c Tr∆∞ng / Create All Features

```python
from feature_engineering import engineer_features

# T·∫°o t·∫•t c·∫£ ƒë·∫∑c tr∆∞ng / Engineer all features
df_engineered, encoders = engineer_features(df_clean, encode_categoricals=True)

print(f"\nT·ªïng s·ªë ƒë·∫∑c tr∆∞ng / Total features: {len(df_engineered.columns)}")
print("\nC√°c ƒë·∫∑c tr∆∞ng m·ªõi / New features:")
for col in df_engineered.columns:
    if col not in df_clean.columns:
        print(f"  - {col}")
```

---

## B√†i 3: Hu·∫•n Luy·ªán M√¥ H√¨nh C∆° B·∫£n / Lesson 3: Basic Model Training

### M·ª•c Ti√™u / Objectives:
- Hu·∫•n luy·ªán m√¥ h√¨nh ƒë∆°n gi·∫£n / Train a simple model
- ƒê√°nh gi√° hi·ªáu su·∫•t / Evaluate performance
- Hi·ªÉu metrics / Understand metrics

### B∆∞·ªõc 3.1: Chu·∫©n B·ªã D·ªØ Li·ªáu / Prepare Data

```python
from model_TranMinhHieu import DisasterResponseModel

# Kh·ªüi t·∫°o m√¥ h√¨nh / Initialize model
model = DisasterResponseModel(model_type='random_forest', task='regression')

# Ch·ªçn ƒë·∫∑c tr∆∞ng / Select features
feature_cols = [
    'affected_population',
    'casualties', 
    'economic_impact_usd',
    'response_time_hours'
]

# Chu·∫©n b·ªã d·ªØ li·ªáu / Prepare data
X, y = model.prepare_features(df_engineered, 'response_effectiveness', feature_cols)

print(f"\nK√≠ch th∆∞·ªõc X / X shape: {X.shape}")
print(f"K√≠ch th∆∞·ªõc y / y shape: {y.shape}")
print(f"\nƒê·∫∑c tr∆∞ng s·ª≠ d·ª•ng / Features used:")
for i, feat in enumerate(feature_cols, 1):
    print(f"  {i}. {feat}")
```

### B∆∞·ªõc 3.2: Hu·∫•n Luy·ªán / Training

```python
# Hu·∫•n luy·ªán m√¥ h√¨nh / Train model
results = model.train(X, y, test_size=0.2, scale_features=True)

# Hi·ªÉn th·ªã k·∫øt qu·∫£ / Display results
print("\n" + "="*50)
print("K·∫æT QU·∫¢ HU·∫§N LUY·ªÜN / TRAINING RESULTS")
print("="*50)
print(f"Training Score (R¬≤): {results['train_score']:.4f}")
print(f"Testing Score (R¬≤):  {results['test_score']:.4f}")

# Gi·∫£i th√≠ch / Explanation
if results['test_score'] > 0.8:
    print("\n‚úÖ M√¥ h√¨nh r·∫•t t·ªët! / Excellent model!")
elif results['test_score'] > 0.6:
    print("\n‚úì M√¥ h√¨nh t·ªët / Good model!")
else:
    print("\n‚ö† M√¥ h√¨nh c·∫ßn c·∫£i thi·ªán / Model needs improvement")
```

### B∆∞·ªõc 3.3: D·ª± ƒêo√°n / Prediction

```python
import numpy as np

# L·∫•y d·ªØ li·ªáu test / Get test data
X_test = results['X_test']
y_test = results['y_test']

# D·ª± ƒëo√°n / Predict
y_pred = model.predict(X_test)

# So s√°nh m·ªôt v√†i gi√° tr·ªã / Compare some values
print("\nSo s√°nh Th·ª±c t·∫ø vs D·ª± ƒëo√°n / Actual vs Predicted:")
print("="*50)
for i in range(min(5, len(y_test))):
    actual = y_test.iloc[i] if hasattr(y_test, 'iloc') else y_test[i]
    predicted = y_pred[i]
    error = abs(actual - predicted)
    print(f"Sample {i+1}: Actual={actual:.3f}, Predicted={predicted:.3f}, Error={error:.3f}")
```

---

## B√†i 4: ƒê√°nh Gi√° M√¥ H√¨nh / Lesson 4: Model Evaluation

### M·ª•c Ti√™u / Objectives:
- S·ª≠ d·ª•ng c√°c metrics ƒë√°nh gi√° / Use evaluation metrics
- T·∫°o visualizations / Create visualizations
- Hi·ªÉu k·∫øt qu·∫£ / Understand results

### B∆∞·ªõc 4.1: T√≠nh Metrics / Calculate Metrics

```python
from evaluation import evaluate_regression_model

# ƒê√°nh gi√° / Evaluate
metrics = evaluate_regression_model(y_test, y_pred, model_name='Random Forest')

# Gi·∫£i th√≠ch metrics / Explain metrics
print("\n" + "="*50)
print("GI·∫¢I TH√çCH METRICS / METRICS EXPLANATION")
print("="*50)
print(f"""
MSE (Mean Squared Error): {metrics['MSE']:.4f}
  ‚Üí Sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh / Average squared error
  ‚Üí C√†ng nh·ªè c√†ng t·ªët / Lower is better

RMSE (Root MSE): {metrics['RMSE']:.4f}
  ‚Üí CƒÉn b·∫≠c hai c·ªßa MSE / Square root of MSE
  ‚Üí C√πng ƒë∆°n v·ªã v·ªõi target / Same unit as target

MAE (Mean Absolute Error): {metrics['MAE']:.4f}
  ‚Üí Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh / Average absolute error
  ‚Üí D·ªÖ hi·ªÉu h∆°n MSE / More interpretable than MSE

R¬≤ Score: {metrics['R2']:.4f}
  ‚Üí T·ª∑ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch / Variance explained
  ‚Üí T·ª´ 0 ƒë·∫øn 1, c√†ng cao c√†ng t·ªët / From 0 to 1, higher is better

MAPE: {metrics['MAPE']:.2f}%
  ‚Üí Sai s·ªë ph·∫ßn trƒÉm / Percentage error
  ‚Üí D·ªÖ hi·ªÉu nh·∫•t / Most interpretable
""")
```

### B∆∞·ªõc 4.2: Visualizations

```python
from evaluation import plot_regression_results
import matplotlib.pyplot as plt

# T·∫°o plots / Create plots
plot_regression_results(y_test, y_pred, model_name='Random Forest', save_fig=True)

print("\n‚úì Plots ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o outputs/ / Plots saved to outputs/")
```

### B∆∞·ªõc 4.3: Feature Importance

```python
# L·∫•y feature importance / Get feature importance
importance_df = model.get_feature_importance(top_n=10)

# V·∫Ω bi·ªÉu ƒë·ªì / Plot
from evaluation import plot_feature_importance
plot_feature_importance(importance_df, model_name='Random Forest', save_fig=True)

# Ph√¢n t√≠ch / Analysis
print("\nPH√ÇN T√çCH ƒê·ªò QUAN TR·ªåNG / IMPORTANCE ANALYSIS:")
print("="*50)
top_feature = importance_df.iloc[0]
print(f"ƒê·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t / Most important feature: {top_feature['feature']}")
print(f"ƒê·ªô quan tr·ªçng / Importance: {top_feature['importance']:.4f}")
```

---

## B√†i 5: So S√°nh Nhi·ªÅu M√¥ H√¨nh / Lesson 5: Comparing Multiple Models

### M·ª•c Ti√™u / Objectives:
- Hu·∫•n luy·ªán nhi·ªÅu m√¥ h√¨nh / Train multiple models
- So s√°nh hi·ªáu su·∫•t / Compare performance
- Ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t / Select best model

### B∆∞·ªõc 5.1: Hu·∫•n Luy·ªán Nhi·ªÅu M√¥ H√¨nh / Train Multiple Models

```python
from model_TranMinhHieu import train_multiple_models

# Hu·∫•n luy·ªán t·∫•t c·∫£ m√¥ h√¨nh / Train all models
models, results = train_multiple_models(X, y, task='regression')

print("\nƒê√É HU·∫§N LUY·ªÜN C√ÅC M√î H√åNH / MODELS TRAINED:")
for model_name in models.keys():
    print(f"  ‚úì {model_name}")
```

### B∆∞·ªõc 5.2: So S√°nh / Compare

```python
from evaluation import compare_models
import pandas as pd

# So s√°nh / Compare
comparison_df = compare_models(results)

# Hi·ªÉn th·ªã b·∫£ng so s√°nh / Display comparison table
print("\nB·∫¢NG SO S√ÅNH M√î H√åNH / MODEL COMPARISON TABLE:")
print("="*60)
print(comparison_df.to_string(index=False))

# T√¨m m√¥ h√¨nh t·ªët nh·∫•t / Find best model
best_idx = comparison_df['Test Score'].idxmax()
best_model = comparison_df.loc[best_idx, 'Model']
best_score = comparison_df.loc[best_idx, 'Test Score']

print("\n" + "="*60)
print(f"M√î H√åNH T·ªëT NH·∫§T / BEST MODEL: {best_model}")
print(f"Test Score: {best_score:.4f}")
print("="*60)
```

### B∆∞·ªõc 5.3: L∆∞u M√¥ H√¨nh T·ªët Nh·∫•t / Save Best Model

```python
# L·∫•y m√¥ h√¨nh t·ªët nh·∫•t / Get best model
best_model_obj = models[best_model.lower().replace(' ', '_')]

# L∆∞u / Save
best_model_obj.save_model(f'models/best_model_{best_model.replace(" ", "_")}.pkl')
print(f"\n‚úì ƒê√£ l∆∞u m√¥ h√¨nh t·ªët nh·∫•t / Best model saved!")
```

---

## B√†i 6: S·ª≠ D·ª•ng Web Dashboard / Lesson 6: Using Web Dashboard

### M·ª•c Ti√™u / Objectives:
- Kh·ªüi ƒë·ªông web app / Launch web app
- Kh√°m ph√° c√°c t√≠nh nƒÉng / Explore features
- S·ª≠ d·ª•ng c√°c c√¥ng c·ª• t∆∞∆°ng t√°c / Use interactive tools

### B∆∞·ªõc 6.1: Kh·ªüi ƒê·ªông / Launch

```bash
# Trong terminal / In terminal
streamlit run web/app.py
```

### B∆∞·ªõc 6.2: C√°c T√≠nh NƒÉng Ch√≠nh / Main Features

**Overview Page:**
- Xem t·ªïng quan metrics / View overall metrics
- Ki·ªÉm tra th·ªëng k√™ nhanh / Check quick statistics
- Xem thi√™n tai g·∫ßn ƒë√¢y / View recent disasters

**Data Explorer:**
```
1. Ch·ªçn nƒÉm quan t√¢m / Select years of interest
2. L·ªçc theo lo·∫°i thi√™n tai / Filter by disaster type
3. Ch·ªçn khu v·ª±c / Select regions
4. Xem d·ªØ li·ªáu ƒë√£ l·ªçc / View filtered data
5. T·∫£i xu·ªëng CSV / Download CSV
```

**Visualizations:**
- Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi / Distribution charts
- Xu h∆∞·ªõng theo th·ªùi gian / Temporal trends
- Ph√¢n t√≠ch khu v·ª±c / Regional analysis
- ƒê√°nh gi√° t√°c ƒë·ªông / Impact assessment

**Statistics:**
- Th·ªëng k√™ m√¥ t·∫£ / Descriptive statistics
- Ma tr·∫≠n t∆∞∆°ng quan / Correlation matrix
- Ph√¢n t√≠ch ph√¢n ph·ªëi / Distribution analysis

**Predictions:**
```python
# Nh·∫≠p c√°c tham s·ªë:
- Lo·∫°i thi√™n tai / Disaster type
- Khu v·ª±c / Region
- D√¢n s·ªë b·ªã ·∫£nh h∆∞·ªüng / Affected population
- Casualties
- T√°c ƒë·ªông kinh t·∫ø / Economic impact
- Th·ªùi gian ph·∫£n ·ª©ng / Response time

# Nh·∫≠n k·∫øt qu·∫£ d·ª± ƒëo√°n / Get prediction result
```

---

## B√†i 7: T√πy Ch·ªânh v√† M·ªü R·ªông / Lesson 7: Customization and Extension

### M·ª•c Ti√™u / Objectives:
- Th√™m d·ªØ li·ªáu m·ªõi / Add new data
- T√πy ch·ªânh m√¥ h√¨nh / Customize models
- T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi / Create new features

### B∆∞·ªõc 7.1: Th√™m D·ªØ Li·ªáu M·ªõi / Add New Data

```python
import pandas as pd

# T·∫°o d·ªØ li·ªáu m·ªõi / Create new data
new_data = pd.DataFrame({
    'date': ['2024-12-01'],
    'disaster_type': ['Earthquake'],
    'country': ['Japan'],
    'region': ['Asia'],
    'affected_population': [100000],
    'casualties': [200],
    'response_time_hours': [18],
    'response_effectiveness': [0.88],
    'economic_impact_usd': [3000000000]
})

# Th√™m v√†o d·ªØ li·ªáu hi·ªán t·∫°i / Append to existing data
df_combined = pd.concat([df, new_data], ignore_index=True)

# L∆∞u / Save
df_combined.to_csv('data/global_disaster_response_updated.csv', index=False)
print(f"‚úì ƒê√£ th√™m {len(new_data)} d√≤ng m·ªõi / Added {len(new_data)} new rows")
```

### B∆∞·ªõc 7.2: T·∫°o ƒê·∫∑c Tr∆∞ng T√πy Ch·ªânh / Create Custom Features

```python
def create_custom_feature(df):
    """T·∫°o ƒë·∫∑c tr∆∞ng t√πy ch·ªânh / Create custom feature"""
    
    # V√≠ d·ª•: T·ª∑ l·ªá hi·ªáu qu·∫£/chi ph√≠ / Example: Efficiency/cost ratio
    df['efficiency_cost_ratio'] = (
        df['response_effectiveness'] / 
        (df['economic_impact_usd'] / 1e9 + 1)  # Tr√°nh chia 0 / Avoid division by 0
    )
    
    # V√≠ d·ª•: Ch·ªâ s·ªë ph·∫£n ·ª©ng nhanh / Example: Fast response indicator
    df['fast_response'] = (df['response_time_hours'] <= 24).astype(int)
    
    return df

# √Åp d·ª•ng / Apply
df_custom = create_custom_feature(df_engineered.copy())
print("‚úì ƒê√£ t·∫°o ƒë·∫∑c tr∆∞ng t√πy ch·ªânh / Custom features created:")
print("  - efficiency_cost_ratio")
print("  - fast_response")
```

### B∆∞·ªõc 7.3: T√πy Ch·ªânh Hyperparameters

```python
from sklearn.ensemble import RandomForestRegressor

# T·∫°o m√¥ h√¨nh v·ªõi tham s·ªë t√πy ch·ªânh / Create model with custom parameters
custom_rf = RandomForestRegressor(
    n_estimators=200,           # S·ªë c√¢y / Number of trees
    max_depth=15,               # ƒê·ªô s√¢u t·ªëi ƒëa / Max depth
    min_samples_split=10,       # Min samples to split
    min_samples_leaf=4,         # Min samples in leaf
    max_features='sqrt',        # Features per split
    random_state=42,
    n_jobs=-1                   # D√πng t·∫•t c·∫£ CPU / Use all CPUs
)

# Hu·∫•n luy·ªán / Train
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
custom_rf.fit(X_train, y_train)

# ƒê√°nh gi√° / Evaluate
score = custom_rf.score(X_test, y_test)
print(f"\nCustom Model Test Score: {score:.4f}")
```

---

## B√†i 8: Best Practices v√† Tips

### M·ª•c Ti√™u / Objectives:
- H·ªçc c√°c best practices / Learn best practices
- T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t / Optimize performance
- Tr√°nh l·ªói th∆∞·ªùng g·∫∑p / Avoid common mistakes

### Best Practices:

**1. Lu√¥n ki·ªÉm tra d·ªØ li·ªáu / Always check data:**
```python
# Ki·ªÉm tra missing values / Check missing values
print(df.isnull().sum())

# Ki·ªÉm tra duplicates / Check duplicates
print(f"Duplicates: {df.duplicated().sum()}")

# Ki·ªÉm tra data types / Check data types
print(df.dtypes)
```

**2. Chia d·ªØ li·ªáu ƒë√∫ng c√°ch / Split data properly:**
```python
# Lu√¥n d√πng random_state ƒë·ªÉ reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    random_state=42  # ‚Üê Quan tr·ªçng! / Important!
)
```

**3. Scale features khi c·∫ßn / Scale features when needed:**
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Ch·ªâ transform, kh√¥ng fit! / Only transform, don't fit!
```

**4. Cross-validation:**
```python
# Lu√¥n d√πng cross-validation ƒë·ªÉ ƒë√°nh gi√° / Always use cross-validation
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"CV Score: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})")
```

**5. L∆∞u v√† load models:**
```python
# L∆∞u c·∫£ scaler v√† model / Save both scaler and model
import pickle

# L∆∞u / Save
with open('models/full_pipeline.pkl', 'wb') as f:
    pickle.dump({'model': model, 'scaler': scaler}, f)

# Load / Load
with open('models/full_pipeline.pkl', 'rb') as f:
    pipeline = pickle.load(f)
    loaded_model = pipeline['model']
    loaded_scaler = pipeline['scaler']
```

### Common Mistakes to Avoid:

**‚ùå Kh√¥ng n√™n:**
```python
# 1. Fit scaler tr√™n to√†n b·ªô d·ªØ li·ªáu / Don't fit scaler on all data
scaler.fit(X)  # ‚ùå Data leakage!

# 2. Qu√™n random_state / Forget random_state
train_test_split(X, y, test_size=0.2)  # ‚ùå Kh√¥ng reproducible / Not reproducible

# 3. D√πng test set ƒë·ªÉ tune / Use test set for tuning
# Tune tr√™n validation set, ch·ªâ d√πng test set cu·ªëi c√πng / Tune on validation, use test at end
```

**‚úÖ N√™n:**
```python
# 1. Fit scaler ch·ªâ tr√™n training data / Fit scaler only on training data
scaler.fit(X_train)  # ‚úÖ

# 2. Lu√¥n d√πng random_state / Always use random_state
train_test_split(X, y, test_size=0.2, random_state=42)  # ‚úÖ

# 3. D√πng validation set ri√™ng / Use separate validation set
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5)
```

---

## B√†i 9: Troubleshooting Guide

### V·∫•n ƒê·ªÅ Th∆∞·ªùng G·∫∑p / Common Issues:

**1. ImportError: No module named 'xyz'**
```bash
# Gi·∫£i ph√°p / Solution:
pip install -r requirements.txt
```

**2. FileNotFoundError: data/xxx.csv**
```python
# Ki·ªÉm tra path / Check path
import os
print(os.getcwd())  # Xem th∆∞ m·ª•c hi·ªán t·∫°i / See current directory

# Di chuy·ªÉn ƒë·∫øn ƒë√∫ng th∆∞ m·ª•c / Navigate to correct directory
cd project_TranMinhHieu
```

**3. Model performance k√©m / Poor model performance**
```python
# Gi·∫£i ph√°p / Solutions:
# 1. Th√™m features / Add more features
# 2. Tune hyperparameters
# 3. Th·ª≠ m√¥ h√¨nh kh√°c / Try different models
# 4. Thu th·∫≠p th√™m d·ªØ li·ªáu / Collect more data
```

**4. Memory Error**
```python
# Gi·∫£i ph√°p / Solutions:
# 1. Gi·∫£m n_estimators trong Random Forest
model = RandomForestRegressor(n_estimators=50)  # Thay v√¨ 100 / Instead of 100

# 2. D√πng fewer features
feature_cols = feature_cols[:5]  # Ch·ªâ d√πng 5 features ƒë·∫ßu / Only use first 5

# 3. Downsample data
df_sample = df.sample(frac=0.5)  # D√πng 50% data
```

**5. Streamlit kh√¥ng kh·ªüi ƒë·ªông / Streamlit won't start**
```bash
# Gi·∫£i ph√°p / Solutions:
# 1. Ki·ªÉm tra port / Check port
streamlit run web/app.py --server.port 8502

# 2. Clear cache / X√≥a cache
streamlit cache clear

# 3. Reinstall / C√†i l·∫°i
pip uninstall streamlit
pip install streamlit
```

---

## T·ªïng K·∫øt / Conclusion

B·∫°n ƒë√£ h·ªçc ƒë∆∞·ª£c / You have learned:

‚úÖ T·∫£i v√† x·ª≠ l√Ω d·ªØ li·ªáu / Load and process data
‚úÖ T·∫°o ƒë·∫∑c tr∆∞ng k·ªπ thu·∫≠t / Engineer features
‚úÖ Hu·∫•n luy·ªán m√¥ h√¨nh ML / Train ML models
‚úÖ ƒê√°nh gi√° v√† so s√°nh m√¥ h√¨nh / Evaluate and compare models
‚úÖ S·ª≠ d·ª•ng web dashboard / Use web dashboard
‚úÖ T√πy ch·ªânh v√† m·ªü r·ªông / Customize and extend
‚úÖ Best practices / Best practices
‚úÖ Troubleshooting / Troubleshooting

**B∆∞·ªõc ti·∫øp theo / Next steps:**
1. Th·ª±c h√†nh v·ªõi d·ªØ li·ªáu ri√™ng / Practice with your own data
2. Th·ª≠ nghi·ªám c√°c m√¥ h√¨nh kh√°c / Experiment with other models
3. T√πy ch·ªânh dashboard / Customize the dashboard
4. T·∫°o ƒë·∫∑c tr∆∞ng m·ªõi / Create new features

**Happy coding! / Ch√∫c b·∫°n code vui v·∫ª! üöÄ**
