import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import sys
import os

# Th√™m ƒë∆∞·ªùng d·∫´n src v√†o path
sys.path.insert(0, os.path.join(os.path.dirname("project_TranMinhHieu/web/data/global_disaster_response_2018_2024.csv"), '..', 'src'))

from preprocessing import load_data, preprocess_data, get_categorical_features
from eda import perform_eda, get_data_summary
from feature_engineering import engineer_features
from model_TranMinhHieu import (
    prepare_data_for_catboost,
    train_optimized_model,
    get_feature_importance,
    load_model,
    save_model
)
from evaluation import calculate_metrics, plot_actual_vs_predicted

# =========================================================
# C·∫§U H√åNH TRANG
# =========================================================
st.set_page_config(
    page_title="Recovery Days Prediction",
    page_icon="üåç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================
# CSS T√ôY CH·ªàNH
# =========================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-top: 10px;
        padding-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)


# =========================================================
# H√ÄM T·∫†O D·ªÆ LI·ªÜU M·∫™U
# =========================================================
@st.cache_data
def create_sample_data(n_samples: int = 10000) -> pd.DataFrame:
    """T·∫°o d·ªØ li·ªáu m·∫´u cho demo."""
    np.random.seed(42)
    
    countries = ['USA', 'Japan', 'China', 'India', 'Brazil', 'Germany', 'UK', 
                 'France', 'Australia', 'Canada', 'Mexico', 'Indonesia', 
                 'Philippines', 'Bangladesh', 'Pakistan']
    
    disaster_types = ['Earthquake', 'Flood', 'Tornado', 'Hurricane', 'Wildfire',
                      'Tsunami', 'Drought', 'Volcanic Eruption', 'Landslide', 'Storm']
    
    data = {
        'date': pd.date_range('2018-01-01', periods=n_samples, freq='h')[:n_samples],
        'country': np.random.choice(countries, n_samples),
        'disaster_type': np.random.choice(disaster_types, n_samples),
        'severity_index': np.random.uniform(1, 10, n_samples),
        'casualties': np.random.exponential(100, n_samples).astype(int),
        'economic_loss_usd': np.random.exponential(1e6, n_samples),
        'response_time_hours': np.random.exponential(24, n_samples),
        'aid_amount_usd': np.random.exponential(5e5, n_samples),
        'response_efficiency_score': np.random.uniform(0, 1, n_samples),
        'latitude': np.random.uniform(-90, 90, n_samples),
        'longitude': np.random.uniform(-180, 180, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    df['recovery_days'] = (
        10 + 
        df['severity_index'] * 5 + 
        np.log1p(df['economic_loss_usd']) * 0.5 +
        df['response_time_hours'] * 0.3 -
        np.log1p(df['aid_amount_usd']) * 0.2 -
        df['response_efficiency_score'] * 10 +
        np.random.normal(0, 10, n_samples)
    ).clip(lower=1)
    
    return df


# =========================================================
# SIDEBAR
# =========================================================
st.sidebar.title("üåç Recovery Days Prediction")
st.sidebar.markdown("---")

# Ch·ªçn ngu·ªìn d·ªØ li·ªáu
data_source = st.sidebar.radio(
    "üìÅ Ngu·ªìn d·ªØ li·ªáu:",
    ["D·ªØ li·ªáu m·∫´u", "T·∫£i l√™n file CSV"]
)

if data_source == "T·∫£i l√™n file CSV":
    uploaded_file = st.sidebar.file_uploader("Ch·ªçn file CSV", type=['csv'])
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        st.sidebar.warning("Vui l√≤ng t·∫£i l√™n file CSV ho·∫∑c s·ª≠ d·ª•ng d·ªØ li·ªáu m·∫´u")
        df = create_sample_data()
else:
    df = create_sample_data()

st.sidebar.markdown("---")
st.sidebar.markdown("""
### üìä Th√¥ng tin Dataset
""")
st.sidebar.write(f"- S·ªë b·∫£n ghi: {len(df):,}")
st.sidebar.write(f"- S·ªë features: {len(df.columns)}")


# =========================================================
# HEADER CH√çNH
# =========================================================
st.markdown('<h1 class="main-header">üåç D·ª± ƒêo√°n S·ªë Ng√†y Ph·ª•c H·ªìi Sau Th·∫£m H·ªça</h1>', unsafe_allow_html=True)
st.markdown("""
<p style="text-align: center; font-size: 1.1rem; color: #666;">
    Machine Learning Project - S·ª≠ d·ª•ng CatBoost Regressor ƒë·ªÉ d·ª± ƒëo√°n recovery_days
</p>
""", unsafe_allow_html=True)

# =========================================================
# TABS CH√çNH
# =========================================================
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìä T·ªïng quan d·ªØ li·ªáu",
    "üìà Ph√¢n t√≠ch EDA", 
    "ü§ñ Hu·∫•n luy·ªán m√¥ h√¨nh",
    "üéØ D·ª± ƒëo√°n",
    "üìã V·ªÅ Project"
])

# =========================================================
# TAB 1: T·ªîNG QUAN D·ªÆ LI·ªÜU
# =========================================================
with tab1:
    st.header("üìä T·ªïng quan d·ªØ li·ªáu")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("S·ªë b·∫£n ghi", f"{len(df):,}")
    with col2:
        st.metric("S·ªë features", len(df.columns))
    with col3:
        st.metric("Recovery Days (Mean)", f"{df['recovery_days'].mean():.1f}")
    with col4:
        st.metric("Recovery Days (Median)", f"{df['recovery_days'].median():.1f}")
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìã M·∫´u d·ªØ li·ªáu")
        st.dataframe(df.head(10), use_container_width=True)
    
    with col2:
        st.subheader("üìà Th·ªëng k√™ m√¥ t·∫£")
        st.dataframe(df.describe(), use_container_width=True)
    
    st.markdown("---")
    st.subheader("üîç Ki·ªÉu d·ªØ li·ªáu c√°c c·ªôt")
    dtype_df = pd.DataFrame({
        'Column': df.columns,
        'Type': df.dtypes.values,
        'Non-Null': df.notnull().sum().values,
        'Missing': df.isnull().sum().values
    })
    st.dataframe(dtype_df, use_container_width=True)


# =========================================================
# TAB 2: PH√ÇN T√çCH EDA
# =========================================================
with tab2:
    st.header("üìà Ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu (EDA)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Ph√¢n b·ªë Recovery Days")
        fig = px.histogram(df, x='recovery_days', nbins=50, 
                          color_discrete_sequence=['steelblue'])
        fig.update_layout(
            xaxis_title="S·ªë ng√†y ph·ª•c h·ªìi",
            yaxis_title="T·∫ßn su·∫•t"
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Boxplot Recovery Days")
        fig = px.box(df, y='recovery_days', color_discrete_sequence=['steelblue'])
        fig.update_layout(yaxis_title="S·ªë ng√†y ph·ª•c h·ªìi")
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Recovery Days theo Disaster Type")
        fig = px.box(df, x='disaster_type', y='recovery_days', 
                    color='disaster_type')
        fig.update_layout(xaxis_tickangle=-45, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Recovery Days theo Country")
        country_mean = df.groupby('country')['recovery_days'].mean().sort_values(ascending=False)
        fig = px.bar(x=country_mean.index, y=country_mean.values,
                    labels={'x': 'Country', 'y': 'Mean Recovery Days'})
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    st.subheader("Ma tr·∫≠n t∆∞∆°ng quan")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    corr_matrix = df[numeric_cols].corr()
    fig = px.imshow(corr_matrix, text_auto='.2f', aspect='auto',
                   color_continuous_scale='RdBu_r')
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Scatter: Severity vs Recovery Days")
        fig = px.scatter(df.sample(min(1000, len(df))), 
                        x='severity_index', y='recovery_days',
                        opacity=0.5)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Scatter: Economic Loss vs Recovery Days")
        df_sample = df.sample(min(1000, len(df)))
        fig = px.scatter(df_sample, 
                        x=np.log1p(df_sample['economic_loss_usd']), 
                        y='recovery_days',
                        opacity=0.5,
                        labels={'x': 'Log(Economic Loss USD)'})
        st.plotly_chart(fig, use_container_width=True)


# =========================================================
# TAB 3: HU·∫§N LUY·ªÜN M√î H√åNH
# =========================================================
with tab3:
    st.header("ü§ñ Hu·∫•n luy·ªán m√¥ h√¨nh CatBoost")
    
    st.markdown("""
    ### M√¥ h√¨nh: CatBoost Regressor
    
    **L√Ω do ch·ªçn CatBoost:**
    - ‚úÖ X·ª≠ l√Ω t·ªët bi·∫øn ph√¢n lo·∫°i (country, disaster_type)
    - ‚úÖ Kh√¥ng c·∫ßn One-Hot Encoding
    - ‚úÖ Hi·ªáu su·∫•t cao v·ªõi dataset v·ª´a-l·ªõn
    - ‚úÖ √çt overfitting v·ªõi Ordered Boosting
    """)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        test_size = st.slider("Test size (%)", 10, 40, 20) / 100
        iterations = st.slider("S·ªë iterations", 100, 1000, 300)
        learning_rate = st.select_slider("Learning rate", 
                                        options=[0.01, 0.03, 0.05, 0.1, 0.2],
                                        value=0.1)
    
    with col2:
        depth = st.slider("Depth", 4, 12, 6)
        l2_leaf_reg = st.slider("L2 regularization", 1, 10, 3)
    
    if st.button("üöÄ Hu·∫•n luy·ªán m√¥ h√¨nh", type="primary"):
        with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh..."):
            # Ti·ªÅn x·ª≠ l√Ω
            X, y = preprocess_data(df, target_column='recovery_days')
            X = engineer_features(X)
            cat_features = get_categorical_features(X)
            
            # Chia d·ªØ li·ªáu
            X_train, X_test, y_train, y_test, cat_indices = prepare_data_for_catboost(
                X, y, cat_features, test_size=test_size, random_state=42
            )
            
            # Hu·∫•n luy·ªán
            params = {
                'iterations': iterations,
                'learning_rate': learning_rate,
                'depth': depth,
                'l2_leaf_reg': l2_leaf_reg
            }
            
            model = train_optimized_model(
                X_train, y_train,
                cat_features=cat_indices,
                params=params
            )
            
            # D·ª± ƒëo√°n v√† ƒë√°nh gi√°
            y_pred = model.predict(X_test)
            metrics = calculate_metrics(y_test.values, y_pred)
            
            # L∆∞u v√†o session state
            st.session_state['model'] = model
            st.session_state['X'] = X
            st.session_state['cat_features'] = cat_features
            
            st.success("‚úÖ ƒê√£ hu·∫•n luy·ªán xong m√¥ h√¨nh!")
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.markdown("### üìä K·∫øt qu·∫£ ƒë√°nh gi√°")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("MAE", f"{metrics['MAE']:.2f}")
            with col2:
                st.metric("RMSE", f"{metrics['RMSE']:.2f}")
            with col3:
                st.metric("R¬≤ Score", f"{metrics['R2']:.4f}")
            with col4:
                st.metric("MAPE", f"{metrics['MAPE']:.2f}%")
            
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Actual vs Predicted")
                fig = px.scatter(x=y_test, y=y_pred, opacity=0.3,
                               labels={'x': 'Actual', 'y': 'Predicted'})
                fig.add_trace(go.Scatter(
                    x=[y_test.min(), y_test.max()],
                    y=[y_test.min(), y_test.max()],
                    mode='lines',
                    name='Perfect Prediction',
                    line=dict(color='red', dash='dash')
                ))
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.subheader("Feature Importance")
                importance_df = get_feature_importance(model, X.columns.tolist())
                fig = px.bar(importance_df.head(15), x='importance', y='feature',
                           orientation='h')
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)


# =========================================================
# TAB 4: D·ª∞ ƒêO√ÅN
# =========================================================
with tab4:
    st.header("üéØ D·ª± ƒëo√°n s·ªë ng√†y ph·ª•c h·ªìi")
    
    st.markdown("""
    Nh·∫≠p th√¥ng tin v·ªÅ th·∫£m h·ªça ƒë·ªÉ d·ª± ƒëo√°n s·ªë ng√†y ph·ª•c h·ªìi:
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        country = st.selectbox("Qu·ªëc gia", df['country'].unique())
        disaster_type = st.selectbox("Lo·∫°i th·∫£m h·ªça", df['disaster_type'].unique())
        severity_index = st.slider("Ch·ªâ s·ªë nghi√™m tr·ªçng (1-10)", 1.0, 10.0, 5.0)
        casualties = st.number_input("S·ªë th∆∞∆°ng vong", 0, 10000, 100)
        economic_loss = st.number_input("Thi·ªát h·∫°i kinh t·∫ø (USD)", 0, 100000000, 1000000)
    
    with col2:
        response_time = st.slider("Th·ªùi gian ph·∫£n ·ª©ng (gi·ªù)", 1.0, 168.0, 24.0)
        aid_amount = st.number_input("S·ªë ti·ªÅn vi·ªán tr·ª£ (USD)", 0, 50000000, 500000)
        efficiency_score = st.slider("ƒêi·ªÉm hi·ªáu qu·∫£ ph·∫£n ·ª©ng (0-1)", 0.0, 1.0, 0.5)
        latitude = st.slider("Vƒ© ƒë·ªô", -90.0, 90.0, 0.0)
        longitude = st.slider("Kinh ƒë·ªô", -180.0, 180.0, 0.0)
    
    if st.button("üîÆ D·ª± ƒëo√°n", type="primary"):
        if 'model' not in st.session_state:
            st.warning("‚ö†Ô∏è Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc (Tab 'Hu·∫•n luy·ªán m√¥ h√¨nh')")
        else:
            # T·∫°o input data
            input_data = pd.DataFrame({
                'country': [country],
                'disaster_type': [disaster_type],
                'severity_index': [severity_index],
                'casualties': [casualties],
                'economic_loss_usd': [economic_loss],
                'response_time_hours': [response_time],
                'aid_amount_usd': [aid_amount],
                'response_efficiency_score': [efficiency_score],
                'latitude': [latitude],
                'longitude': [longitude],
                'year': [2024],
                'month': [6]
            })
            
            # Feature engineering
            # Disable time feature creation since we already have year/month from input
            input_data = engineer_features(input_data, create_time=False)
            
            # Log transform
            input_data['economic_loss_usd_log'] = np.log1p(input_data['economic_loss_usd'])
            input_data['aid_amount_usd_log'] = np.log1p(input_data['aid_amount_usd'])
            
            # ƒê·∫£m b·∫£o c√≥ ƒë·ªß c√°c c·ªôt nh∆∞ training data
            X_train = st.session_state['X']
            for col in X_train.columns:
                if col not in input_data.columns:
                    if X_train[col].dtype == 'object':
                        input_data[col] = X_train[col].mode()[0]
                    else:
                        input_data[col] = 0
            
            # S·∫Øp x·∫øp l·∫°i c·ªôt
            input_data = input_data[X_train.columns]
            
            # D·ª± ƒëo√°n
            model = st.session_state['model']
            prediction = model.predict(input_data)[0]
            
            st.markdown("---")
            st.markdown(f"""
            <div style="text-align: center; padding: 2rem; background-color: #e8f4f8; border-radius: 1rem;">
                <h2 style="color: #1E88E5;">K·∫øt qu·∫£ d·ª± ƒëo√°n</h2>
                <h1 style="font-size: 4rem; color: #0D47A1;">{prediction:.1f}</h1>
                <h3>ng√†y ph·ª•c h·ªìi</h3>
            </div>
            """, unsafe_allow_html=True)
            
            # Ph√¢n t√≠ch
            st.markdown("---")
            st.subheader("üìù Ph√¢n t√≠ch")
            
            if prediction < 30:
                st.success("üü¢ D·ª± ki·∫øn ph·ª•c h·ªìi NHANH (< 1 th√°ng)")
            elif prediction < 90:
                st.warning("üü° D·ª± ki·∫øn ph·ª•c h·ªìi TRUNG B√åNH (1-3 th√°ng)")
            else:
                st.error("üî¥ D·ª± ki·∫øn ph·ª•c h·ªìi CH·∫¨M (> 3 th√°ng)")


# =========================================================
# TAB 5: V·ªÄ PROJECT
# =========================================================
with tab5:
    st.header("üìã V·ªÅ Project")
    
    st.markdown("""
    ## D·ª± ƒëo√°n s·ªë ng√†y ph·ª•c h·ªìi sau th·∫£m h·ªça to√†n c·∫ßu
    
    ### 1. Gi·ªõi thi·ªáu
    
    Project n√†y x√¢y d·ª±ng m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± ƒëo√°n **s·ªë ng√†y ph·ª•c h·ªìi (recovery_days)** 
    sau c√°c th·∫£m h·ªça t·ª± nhi√™n tr√™n to√†n c·∫ßu. ƒê√¢y l√† b√†i to√°n **h·ªìi quy (Regression)**.
    
    ### 2. Dataset
    
    - **Ngu·ªìn**: Global Disaster Response 2018-2024
    - **Quy m√¥**: ~50,000 b·∫£n ghi
    - **Bi·∫øn m·ª•c ti√™u**: recovery_days
    
    ### 3. M√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn: CatBoost Regressor
    
    **L√Ω do ch·ªçn:**
    - ‚úÖ X·ª≠ l√Ω t·ªët bi·∫øn ph√¢n lo·∫°i (country, disaster_type) - kh√¥ng c·∫ßn One-Hot Encoding
    - ‚úÖ B·∫Øt ƒë∆∞·ª£c quan h·ªá phi tuy·∫øn gi·ªØa c√°c bi·∫øn
    - ‚úÖ Hi·ªáu su·∫•t cao v·ªõi dataset v·ª´a-l·ªõn (50k d√≤ng)
    - ‚úÖ √çt overfitting nh·ªù Ordered Boosting
    - ‚úÖ H·ªó tr·ª£ gi·∫£i th√≠ch m√¥ h√¨nh (Feature Importance, SHAP)
    
    ### 4. Pipeline
    
    ```
    Data ‚Üí Preprocessing ‚Üí EDA ‚Üí Feature Engineering ‚Üí Training ‚Üí Evaluation
    ```
    
    ### 5. ƒê√°nh gi√° m√¥ h√¨nh
    
    S·ª≠ d·ª•ng c√°c ch·ªâ s·ªë **h·ªìi quy**:
    - **MAE** (Mean Absolute Error)
    - **RMSE** (Root Mean Squared Error)
    - **R¬≤ Score**
    - **MAPE** (Mean Absolute Percentage Error)
    
    ### 6. T√°c gi·∫£
    
    **Tr·∫ßn Minh Hi·∫øu**
    
    ---
    
    ### üìö T√†i li·ªáu tham kh·∫£o
    
    1. Prokhorenkova et al., *CatBoost: unbiased boosting with categorical features*, NeurIPS, 2018
    2. Lundberg & Lee, *A Unified Approach to Interpreting Model Predictions*, NeurIPS, 2017
    3. EM-DAT: The International Disaster Database
    4. World Bank Open Data
    """)


# =========================================================
# FOOTER
# =========================================================
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #888; padding: 1rem;">
    <p>üìö Machine Learning Project - Recovery Days Prediction</p>
    <p>T√°c gi·∫£: Tr·∫ßn Minh Hi·∫øu | ¬© 2024</p>
</div>
""", unsafe_allow_html=True)
