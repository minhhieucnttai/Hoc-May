{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17034477",
   "metadata": {},
   "source": [
    "# B√ÅO C√ÅO ƒê·ªí √ÅN M√îN H·ªåC M√ÅY\n",
    "## ƒê·ªÅ t√†i: D·ª± ƒëo√°n s·ªë ng√†y h·ªìi ph·ª•c sau th·∫£m h·ªça thi√™n nhi√™n\n",
    "\n",
    "**Sinh vi√™n th·ª±c hi·ªán:** Tr·∫ßn Minh Hi·∫øu  \n",
    "**L·ªõp:** [ƒêi·ªÅn m√£ l·ªõp]  \n",
    "**MSSV:** [ƒêi·ªÅn MSSV]\n",
    "\n",
    "**Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:** [ƒêi·ªÅn t√™n GVHD]\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Gi·ªõi thi·ªáu ƒë·ªÅ t√†i\n",
    "\n",
    "ƒê·ªÅ t√†i s·ª≠ d·ª•ng thu·∫≠t to√°n CatBoost Regressor ƒë·ªÉ d·ª± ƒëo√°n s·ªë ng√†y h·ªìi ph·ª•c (`recovery_days`) sau c√°c th·∫£m h·ªça thi√™n nhi√™n d·ª±a tr√™n c√°c ƒë·∫∑c tr∆∞ng nh∆∞: qu·ªëc gia, lo·∫°i th·∫£m h·ªça, m·ª©c ƒë·ªô nghi√™m tr·ªçng, thi·ªát h·∫°i kinh t·∫ø, v.v.\n",
    "\n",
    "### 2. L√Ω do ch·ªçn thu·∫≠t to√°n CatBoost\n",
    "\n",
    "- X·ª≠ l√Ω t·ªët bi·∫øn ph√¢n lo·∫°i (categorical features) m√† kh√¥ng c·∫ßn m√£ h√≥a\n",
    "- Hi·ªáu su·∫•t cao v·ªõi t·∫≠p d·ªØ li·ªáu l·ªõn\n",
    "- S·ª≠ d·ª•ng k·ªπ thu·∫≠t Ordered Boosting gi√∫p gi·∫£m overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b70a505",
   "metadata": {},
   "source": [
    "## 3. Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295df34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# C√†i ƒë·∫∑t th∆∞ vi·ªán CatBoost (ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn)\n",
    "%pip install catboost -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8fe449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import th∆∞ vi·ªán th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Import th∆∞ vi·ªán th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4965f6e1",
   "metadata": {},
   "source": [
    "## 4. ƒê·ªçc v√† kh√°m ph√° d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3b33e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/global_disaster_response_2018_2024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ƒê·ªçc file d·ªØ li·ªáu\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/global_disaster_response_2018_2024.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Xem th√¥ng tin c∆° b·∫£n\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mK√≠ch th∆∞·ªõc d·ªØ li·ªáu:\u001b[39m\u001b[33m\"\u001b[39m, df.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/global_disaster_response_2018_2024.csv'"
     ]
    }
   ],
   "source": [
    "# ƒê·ªçc file d·ªØ li·ªáu\n",
    "df = pd.read_csv('../data/global_disaster_response_2018_2024.csv')\n",
    "\n",
    "# Xem th√¥ng tin c∆° b·∫£n\n",
    "print(\"K√≠ch th∆∞·ªõc d·ªØ li·ªáu:\", df.shape)\n",
    "print(\"\\nDanh s√°ch c√°c c·ªôt:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\n5 d√≤ng ƒë·∫ßu ti√™n:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03989a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xem th√¥ng tin ki·ªÉu d·ªØ li·ªáu\n",
    "print(\"Th√¥ng tin chi ti·∫øt:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nTh·ªëng k√™ m√¥ t·∫£:\")\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra d·ªØ li·ªáu thi·∫øu (missing values)\n",
    "print(\"Ki·ªÉm tra gi√° tr·ªã thi·∫øu:\")\n",
    "missing = df.isnull().sum()\n",
    "print(missing)\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"\\n=> D·ªØ li·ªáu kh√¥ng c√≥ gi√° tr·ªã thi·∫øu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f31aae",
   "metadata": {},
   "source": [
    "## 5. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (Data Preprocessing)\n",
    "\n",
    "### 5.1. X·ª≠ l√Ω bi·∫øn th·ªùi gian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e67c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o b·∫£n sao d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Chuy·ªÉn c·ªôt date sang ki·ªÉu datetime\n",
    "df_processed['date'] = pd.to_datetime(df_processed['date'], format='%d/%m/%Y')\n",
    "\n",
    "# Tr√≠ch xu·∫•t nƒÉm v√† th√°ng t·ª´ c·ªôt date\n",
    "df_processed['year'] = df_processed['date'].dt.year\n",
    "df_processed['month'] = df_processed['date'].dt.month\n",
    "\n",
    "print(\"ƒê√£ x·ª≠ l√Ω bi·∫øn th·ªùi gian:\")\n",
    "print(\"- NƒÉm:\", df_processed['year'].min(), \"-\", df_processed['year'].max())\n",
    "print(\"- Th√°ng: 1 - 12\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c1b1f",
   "metadata": {},
   "source": [
    "### 5.2. X·ª≠ l√Ω bi·∫øn ph√¢n lo·∫°i (Categorical Features)\n",
    "\n",
    "CatBoost c√≥ kh·∫£ nƒÉng x·ª≠ l√Ω tr·ª±c ti·∫øp bi·∫øn ph√¢n lo·∫°i m√† kh√¥ng c·∫ßn One-Hot Encoding hay Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca41ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X√°c ƒë·ªãnh c√°c bi·∫øn ph√¢n lo·∫°i\n",
    "categorical_features = ['country', 'disaster_type']\n",
    "\n",
    "# Xem s·ªë l∆∞·ª£ng gi√° tr·ªã unique c·ªßa m·ªói bi·∫øn\n",
    "for col in categorical_features:\n",
    "    print(f\"{col}:\")\n",
    "    print(f\"  - S·ªë gi√° tr·ªã unique: {df_processed[col].nunique()}\")\n",
    "    print(f\"  - C√°c gi√° tr·ªã: {df_processed[col].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c1545",
   "metadata": {},
   "source": [
    "### 5.3. X·ª≠ l√Ω bi·∫øn s·ªë (Log Transformation)\n",
    "\n",
    "√Åp d·ª•ng ph√©p bi·∫øn ƒë·ªïi log cho c√°c bi·∫øn c√≥ ph√¢n ph·ªëi l·ªách (skewed distribution) ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea91817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°c bi·∫øn c·∫ßn log transform\n",
    "log_transform_cols = ['economic_loss_usd', 'aid_amount_usd']\n",
    "\n",
    "print(\"Log transformation:\")\n",
    "for col in log_transform_cols:\n",
    "    # T√≠nh skewness tr∆∞·ªõc v√† sau khi transform\n",
    "    skew_before = df_processed[col].skew()\n",
    "    df_processed[f'{col}_log'] = np.log1p(df_processed[col])\n",
    "    skew_after = df_processed[f'{col}_log'].skew()\n",
    "    \n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Skewness tr∆∞·ªõc: {skew_before:.3f}\")\n",
    "    print(f\"  - Skewness sau: {skew_after:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edcda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi·ªÉu ƒë·ªì so s√°nh ph√¢n ph·ªëi tr∆∞·ªõc v√† sau log transform\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for idx, col in enumerate(log_transform_cols):\n",
    "    # Ph√¢n ph·ªëi g·ªëc\n",
    "    axes[idx, 0].hist(df_processed[col], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "    axes[idx, 0].set_title(f'Truoc transform: {col}')\n",
    "    axes[idx, 0].set_xlabel(col)\n",
    "    axes[idx, 0].set_ylabel('Tan suat')\n",
    "    \n",
    "    # Ph√¢n ph·ªëi sau log transform\n",
    "    axes[idx, 1].hist(df_processed[f'{col}_log'], bins=50, color='orange', alpha=0.7, edgecolor='black')\n",
    "    axes[idx, 1].set_title(f'Sau log transform: {col}_log')\n",
    "    axes[idx, 1].set_xlabel(f'{col}_log')\n",
    "    axes[idx, 1].set_ylabel('Tan suat')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/log_transform_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cbf75a",
   "metadata": {},
   "source": [
    "### 5.4. Feature Engineering\n",
    "\n",
    "T·∫°o th√™m c√°c ƒë·∫∑c tr∆∞ng m·ªõi t·ª´ c√°c bi·∫øn c√≥ s·∫µn ƒë·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2310d399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o c√°c ƒë·∫∑c tr∆∞ng m·ªõi\n",
    "print(\"T·∫°o c√°c ƒë·∫∑c tr∆∞ng m·ªõi:\")\n",
    "\n",
    "# 1. Thi·ªát h·∫°i kinh t·∫ø tr√™n m·ªói th∆∞∆°ng vong\n",
    "df_processed['loss_per_casualty'] = df_processed['economic_loss_usd'] / (df_processed['casualties'] + 1)\n",
    "print(\"1. loss_per_casualty = economic_loss_usd / (casualties + 1)\")\n",
    "\n",
    "# 2. Ti·ªÅn vi·ªán tr·ª£ tr√™n m·ªói gi·ªù ph·∫£n ·ª©ng\n",
    "df_processed['aid_per_hour'] = df_processed['aid_amount_usd'] / (df_processed['response_time_hours'] + 1)\n",
    "print(\"2. aid_per_hour = aid_amount_usd / (response_time_hours + 1)\")\n",
    "\n",
    "# 3. T·ª∑ l·ªá m·ª©c ƒë·ªô nghi√™m tr·ªçng / hi·ªáu qu·∫£ ph·∫£n ·ª©ng\n",
    "df_processed['severity_response_ratio'] = df_processed['severity_index'] / (df_processed['response_efficiency_score'] + 1) * 100\n",
    "print(\"3. severity_response_ratio = severity_index / response_efficiency_score * 100\")\n",
    "\n",
    "# 4. Hi·ªáu qu·∫£ vi·ªán tr·ª£\n",
    "df_processed['aid_efficiency'] = df_processed['aid_amount_usd'] / (df_processed['economic_loss_usd'] + 1)\n",
    "print(\"4. aid_efficiency = aid_amount_usd / economic_loss_usd\")\n",
    "\n",
    "print(\"\\n=> ƒê√£ t·∫°o 4 ƒë·∫∑c tr∆∞ng m·ªõi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cc6e2",
   "metadata": {},
   "source": [
    "## 6. Ph√¢n t√≠ch bi·∫øn m·ª•c ti√™u (Target Variable)\n",
    "\n",
    "Bi·∫øn m·ª•c ti√™u c·∫ßn d·ª± ƒëo√°n l√† `recovery_days` - s·ªë ng√†y h·ªìi ph·ª•c sau th·∫£m h·ªça."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fb9615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bi·ªÉu ƒë·ªì ph√¢n ph·ªëi bi·∫øn m·ª•c ti√™u\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_processed['recovery_days'], bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(df_processed['recovery_days'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].set_xlabel('So ngay hoi phuc')\n",
    "axes[0].set_ylabel('Tan suat')\n",
    "axes[0].set_title('Phan phoi Recovery Days')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df_processed['recovery_days'])\n",
    "axes[1].set_ylabel('So ngay hoi phuc')\n",
    "axes[1].set_title('Box Plot')\n",
    "\n",
    "# Density plot\n",
    "df_processed['recovery_days'].plot(kind='kde', ax=axes[2], color='steelblue')\n",
    "axes[2].set_xlabel('So ngay hoi phuc')\n",
    "axes[2].set_title('Mat do phan phoi')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/target_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Th·ªëng k√™ m√¥ t·∫£\n",
    "print(\"Thong ke bien muc tieu recovery_days:\")\n",
    "print(f\"  - Trung binh: {df_processed['recovery_days'].mean():.2f} ngay\")\n",
    "print(f\"  - Trung vi: {df_processed['recovery_days'].median():.2f} ngay\")\n",
    "print(f\"  - Do lech chuan: {df_processed['recovery_days'].std():.2f} ngay\")\n",
    "print(f\"  - Min: {df_processed['recovery_days'].min()} ngay\")\n",
    "print(f\"  - Max: {df_processed['recovery_days'].max()} ngay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed599277",
   "metadata": {},
   "source": [
    "## 7. Chu·∫©n b·ªã d·ªØ li·ªáu cho m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2c818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh s√°ch c√°c ƒë·∫∑c tr∆∞ng (features) s·ª≠ d·ª•ng cho m√¥ h√¨nh\n",
    "feature_cols = [\n",
    "    # Bi·∫øn ph√¢n lo·∫°i\n",
    "    'country', 'disaster_type',\n",
    "    \n",
    "    # Bi·∫øn s·ªë g·ªëc\n",
    "    'severity_index', 'casualties', 'response_time_hours', \n",
    "    'response_efficiency_score', 'latitude', 'longitude',\n",
    "    \n",
    "    # Bi·∫øn ƒë√£ log transform\n",
    "    'economic_loss_usd_log', 'aid_amount_usd_log',\n",
    "    \n",
    "    # Bi·∫øn th·ªùi gian\n",
    "    'year', 'month',\n",
    "    \n",
    "    # ƒê·∫∑c tr∆∞ng m·ªõi\n",
    "    'loss_per_casualty', 'aid_per_hour', 'severity_response_ratio', 'aid_efficiency'\n",
    "]\n",
    "\n",
    "# Bi·∫øn m·ª•c ti√™u\n",
    "target_col = 'recovery_days'\n",
    "\n",
    "print(f\"Tong so features: {len(feature_cols)}\")\n",
    "print(\"\\nDanh sach features:\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    ftype = \"(categorical)\" if col in categorical_features else \"(numerical)\"\n",
    "    print(f\"  {i+1}. {col} {ftype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779267d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T√°ch features (X) v√† target (y)\n",
    "X = df_processed[feature_cols].copy()\n",
    "y = df_processed[target_col].copy()\n",
    "\n",
    "# Chia d·ªØ li·ªáu th√†nh t·∫≠p train v√† test (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Chia du lieu:\")\n",
    "print(f\"  - Tap train: {X_train.shape[0]} mau ({X_train.shape[0]/len(X)*100:.0f}%)\")\n",
    "print(f\"  - Tap test: {X_test.shape[0]} mau ({X_test.shape[0]/len(X)*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff243aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o CatBoost Pool (ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ri√™ng c·ªßa CatBoost)\n",
    "train_pool = Pool(\n",
    "    data=X_train, \n",
    "    label=y_train, \n",
    "    cat_features=categorical_features\n",
    ")\n",
    "\n",
    "test_pool = Pool(\n",
    "    data=X_test, \n",
    "    label=y_test, \n",
    "    cat_features=categorical_features\n",
    ")\n",
    "\n",
    "print(\"Da tao CatBoost Pool voi categorical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8e86e",
   "metadata": {},
   "source": [
    "## 8. X√¢y d·ª±ng v√† hu·∫•n luy·ªán m√¥ h√¨nh CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28321815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kh·ªüi t·∫°o m√¥ h√¨nh CatBoost Regressor\n",
    "model = CatBoostRegressor(\n",
    "    iterations=1000,           # S·ªë l∆∞·ª£ng c√¢y t·ªëi ƒëa\n",
    "    learning_rate=0.05,        # T·ªëc ƒë·ªô h·ªçc\n",
    "    depth=8,                   # ƒê·ªô s√¢u c√¢y\n",
    "    l2_leaf_reg=3,             # H·ªá s·ªë regularization L2\n",
    "    min_data_in_leaf=20,       # S·ªë m·∫´u t·ªëi thi·ªÉu trong leaf node\n",
    "    loss_function='RMSE',      # H√†m m·∫•t m√°t\n",
    "    eval_metric='RMSE',        # Metric ƒë√°nh gi√°\n",
    "    random_seed=42,\n",
    "    verbose=100                # In log m·ªói 100 iterations\n",
    ")\n",
    "\n",
    "print(\"Cac tham so mo hinh:\")\n",
    "print(\"  - iterations: 1000\")\n",
    "print(\"  - learning_rate: 0.05\")\n",
    "print(\"  - depth: 8\")\n",
    "print(\"  - l2_leaf_reg: 3\")\n",
    "print(\"  - loss_function: RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e612644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi early stopping\n",
    "print(\"üöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán CatBoost...\\n\")\n",
    "\n",
    "model.fit(\n",
    "    train_pool,\n",
    "    eval_set=test_pool,\n",
    "    early_stopping_rounds=50,  # D·ª´ng n·∫øu kh√¥ng c·∫£i thi·ªán sau 50 rounds\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Ho√†n th√†nh hu·∫•n luy·ªán!\")\n",
    "print(f\"   - Best iteration: {model.best_iteration_}\")\n",
    "print(f\"   - Best RMSE: {model.best_score_['validation']['RMSE']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e477fc0",
   "metadata": {},
   "source": [
    "## 7. ƒê√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D·ª± ƒëo√°n\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# H√†m t√≠nh c√°c metrics\n",
    "def evaluate_metrics(y_true, y_pred, dataset_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nüìä {dataset_name}:\")\n",
    "    print(f\"   - MAE  (Mean Absolute Error): {mae:.4f} ng√†y\")\n",
    "    print(f\"   - RMSE (Root Mean Squared Error): {rmse:.4f} ng√†y\")\n",
    "    print(f\"   - R¬≤   (R-squared Score): {r2:.4f}\")\n",
    "    \n",
    "    return {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üìà K·∫æT QU·∫¢ ƒê√ÅNH GI√Å M√î H√åNH CATBOOST\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_metrics = evaluate_metrics(y_train, y_train_pred, \"T·∫¨P HU·∫§N LUY·ªÜN (Train)\")\n",
    "test_metrics = evaluate_metrics(y_test, y_test_pred, \"T·∫¨P KI·ªÇM TRA (Test)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"\\nüéØ K·∫øt lu·∫≠n:\")\n",
    "print(f\"   - M√¥ h√¨nh d·ª± ƒëo√°n sai trung b√¨nh {test_metrics['MAE']:.2f} ng√†y\")\n",
    "print(f\"   - M√¥ h√¨nh gi·∫£i th√≠ch ƒë∆∞·ª£c {test_metrics['R2']*100:.2f}% ph∆∞∆°ng sai c·ªßa d·ªØ li·ªáu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8046aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Scatter plot: Actual vs Predicted\n",
    "axes[0].scatter(y_test, y_test_pred, alpha=0.3, color='steelblue', s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Gi√° tr·ªã th·ª±c t·∫ø (ng√†y)', fontsize=12)\n",
    "axes[0].set_ylabel('Gi√° tr·ªã d·ª± ƒëo√°n (ng√†y)', fontsize=12)\n",
    "axes[0].set_title('Actual vs Predicted', fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# 2. Residual plot\n",
    "residuals = y_test - y_test_pred\n",
    "axes[1].scatter(y_test_pred, residuals, alpha=0.3, color='coral', s=10)\n",
    "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Gi√° tr·ªã d·ª± ƒëo√°n (ng√†y)', fontsize=12)\n",
    "axes[1].set_ylabel('Residuals (ng√†y)', fontsize=12)\n",
    "axes[1].set_title('Residual Plot', fontsize=14)\n",
    "\n",
    "# 3. Distribution of residuals\n",
    "axes[2].hist(residuals, bins=50, edgecolor='black', color='green', alpha=0.7)\n",
    "axes[2].axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "axes[2].set_xlabel('Residuals (ng√†y)', fontsize=12)\n",
    "axes[2].set_ylabel('Frequency', fontsize=12)\n",
    "axes[2].set_title(f'Distribution of Residuals\\nMean: {residuals.mean():.2f}, Std: {residuals.std():.2f}', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92338761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance t·ª´ CatBoost\n",
    "feature_importance = model.get_feature_importance(prettified=True)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(feature_importance)))\n",
    "plt.barh(feature_importance['Feature Id'], feature_importance['Importances'], color=colors)\n",
    "plt.xlabel('Importance (%)', fontsize=12)\n",
    "plt.title('üéØ Feature Importance - CatBoost', fontsize=14)\n",
    "plt.gca().invert_yaxis()  # Quan tr·ªçng nh·∫•t ·ªü tr√™n\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Top 10 ƒë·∫∑c tr∆∞ng quan tr·ªçng nh·∫•t:\")\n",
    "print(feature_importance.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096c5fb",
   "metadata": {},
   "source": [
    "## 8. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed36ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation v·ªõi CatBoost\n",
    "from catboost import cv\n",
    "\n",
    "# T·∫°o Pool t·ª´ to√†n b·ªô d·ªØ li·ªáu\n",
    "full_pool = Pool(data=X, label=y, cat_features=categorical_features)\n",
    "\n",
    "# Th·ª±c hi·ªán 5-fold cross-validation\n",
    "print(\"üîÑ Th·ª±c hi·ªán 5-Fold Cross-Validation...\\n\")\n",
    "\n",
    "cv_params = model.get_params()\n",
    "cv_params['iterations'] = model.best_iteration_  # S·ª≠ d·ª•ng best iteration\n",
    "\n",
    "cv_results = cv(\n",
    "    pool=full_pool,\n",
    "    params=cv_params,\n",
    "    fold_count=5,\n",
    "    shuffle=True,\n",
    "    partition_random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"\\nüìä K·∫æT QU·∫¢ CROSS-VALIDATION (5-Fold):\")\n",
    "print(\"=\"*50)\n",
    "final_rmse_mean = cv_results['test-RMSE-mean'].iloc[-1]\n",
    "final_rmse_std = cv_results['test-RMSE-std'].iloc[-1]\n",
    "print(f\"   - RMSE: {final_rmse_mean:.4f} ¬± {final_rmse_std:.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fc497",
   "metadata": {},
   "source": [
    "## 9. H·ªá th·ªëng d·ª± ƒëo√°n s·ªë ng√†y h·ªìi ph·ª•c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8913843c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class d·ª± ƒëo√°n s·ªë ng√†y h·ªìi ph·ª•c\n",
    "class RecoveryDaysPredictor:\n",
    "    def __init__(self, model, feature_cols, categorical_features):\n",
    "        self.model = model\n",
    "        self.feature_cols = feature_cols\n",
    "        self.categorical_features = categorical_features\n",
    "    \n",
    "    def preprocess(self, disaster_data):\n",
    "        \"\"\"\n",
    "        Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o.\n",
    "        \"\"\"\n",
    "        df_input = pd.DataFrame([disaster_data])\n",
    "        \n",
    "        # Log-transform\n",
    "        df_input['economic_loss_usd_log'] = np.log1p(df_input['economic_loss_usd'])\n",
    "        df_input['aid_amount_usd_log'] = np.log1p(df_input['aid_amount_usd'])\n",
    "        \n",
    "        # Feature engineering\n",
    "        df_input['loss_per_casualty'] = df_input['economic_loss_usd'] / (df_input['casualties'] + 1)\n",
    "        df_input['aid_per_hour'] = df_input['aid_amount_usd'] / (df_input['response_time_hours'] + 1)\n",
    "        df_input['severity_response_ratio'] = df_input['severity_index'] / (df_input['response_efficiency_score'] + 1) * 100\n",
    "        df_input['aid_efficiency'] = df_input['aid_amount_usd'] / (df_input['economic_loss_usd'] + 1)\n",
    "        \n",
    "        return df_input[self.feature_cols]\n",
    "    \n",
    "    def predict(self, disaster_data):\n",
    "        \"\"\"\n",
    "        D·ª± ƒëo√°n s·ªë ng√†y h·ªìi ph·ª•c.\n",
    "        \"\"\"\n",
    "        X_input = self.preprocess(disaster_data)\n",
    "        prediction = self.model.predict(X_input)[0]\n",
    "        return max(1, round(prediction, 1))  # T·ªëi thi·ªÉu 1 ng√†y\n",
    "\n",
    "# Kh·ªüi t·∫°o predictor\n",
    "predictor = RecoveryDaysPredictor(model, feature_cols, categorical_features)\n",
    "print(\"‚úÖ H·ªá th·ªëng d·ª± ƒëo√°n ƒë√£ s·∫µn s√†ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388bd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°c qu·ªëc gia v√† lo·∫°i th·∫£m h·ªça c√≥ trong d·ªØ li·ªáu\n",
    "print(\"üåç Danh s√°ch qu·ªëc gia c√≥ trong d·ªØ li·ªáu:\")\n",
    "print(sorted(df['country'].unique()))\n",
    "print(\"\\nüåã Danh s√°ch lo·∫°i th·∫£m h·ªça:\")\n",
    "print(sorted(df['disaster_type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 1: ƒê·ªông ƒë·∫•t t·∫°i Japan\n",
    "disaster_example_1 = {\n",
    "    'country': 'Japan',\n",
    "    'disaster_type': 'Earthquake',\n",
    "    'severity_index': 7.5,\n",
    "    'casualties': 150,\n",
    "    'economic_loss_usd': 5000000,\n",
    "    'response_time_hours': 8,\n",
    "    'aid_amount_usd': 300000,\n",
    "    'response_efficiency_score': 85,\n",
    "    'latitude': 35.6762,\n",
    "    'longitude': 139.6503,\n",
    "    'year': 2024,\n",
    "    'month': 6\n",
    "}\n",
    "\n",
    "prediction_1 = predictor.predict(disaster_example_1)\n",
    "print(\"üåè V√ç D·ª§ 1: ƒê·ªông ƒë·∫•t t·∫°i Japan\")\n",
    "print(f\"   - M·ª©c ƒë·ªô nghi√™m tr·ªçng: {disaster_example_1['severity_index']}\")\n",
    "print(f\"   - S·ªë th∆∞∆°ng vong: {disaster_example_1['casualties']}\")\n",
    "print(f\"   - Thi·ªát h·∫°i kinh t·∫ø: ${disaster_example_1['economic_loss_usd']:,}\")\n",
    "print(f\"   - Th·ªùi gian ph·∫£n ·ª©ng: {disaster_example_1['response_time_hours']} gi·ªù\")\n",
    "print(f\"   üìå D·ª∞ ƒêO√ÅN S·ªê NG√ÄY H·ªíI PH·ª§C: {prediction_1} ng√†y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ba5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 2: B√£o t·∫°i Philippines\n",
    "disaster_example_2 = {\n",
    "    'country': 'Philippines',\n",
    "    'disaster_type': 'Hurricane',\n",
    "    'severity_index': 8.2,\n",
    "    'casualties': 200,\n",
    "    'economic_loss_usd': 8000000,\n",
    "    'response_time_hours': 12,\n",
    "    'aid_amount_usd': 500000,\n",
    "    'response_efficiency_score': 75,\n",
    "    'latitude': 14.5995,\n",
    "    'longitude': 120.9842,\n",
    "    'year': 2024,\n",
    "    'month': 9\n",
    "}\n",
    "\n",
    "prediction_2 = predictor.predict(disaster_example_2)\n",
    "print(\"üåÄ V√ç D·ª§ 2: B√£o t·∫°i Philippines\")\n",
    "print(f\"   - M·ª©c ƒë·ªô nghi√™m tr·ªçng: {disaster_example_2['severity_index']}\")\n",
    "print(f\"   - S·ªë th∆∞∆°ng vong: {disaster_example_2['casualties']}\")\n",
    "print(f\"   - Thi·ªát h·∫°i kinh t·∫ø: ${disaster_example_2['economic_loss_usd']:,}\")\n",
    "print(f\"   - Th·ªùi gian ph·∫£n ·ª©ng: {disaster_example_2['response_time_hours']} gi·ªù\")\n",
    "print(f\"   üìå D·ª∞ ƒêO√ÅN S·ªê NG√ÄY H·ªíI PH·ª§C: {prediction_2} ng√†y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6d7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√≠ d·ª• 3: L≈© l·ª•t t·∫°i India\n",
    "disaster_example_3 = {\n",
    "    'country': 'India',\n",
    "    'disaster_type': 'Flood',\n",
    "    'severity_index': 5.5,\n",
    "    'casualties': 80,\n",
    "    'economic_loss_usd': 3000000,\n",
    "    'response_time_hours': 15,\n",
    "    'aid_amount_usd': 200000,\n",
    "    'response_efficiency_score': 70,\n",
    "    'latitude': 20.5937,\n",
    "    'longitude': 78.9629,\n",
    "    'year': 2024,\n",
    "    'month': 7\n",
    "}\n",
    "\n",
    "prediction_3 = predictor.predict(disaster_example_3)\n",
    "print(\"üåä V√ç D·ª§ 3: L≈© l·ª•t t·∫°i India\")\n",
    "print(f\"   - M·ª©c ƒë·ªô nghi√™m tr·ªçng: {disaster_example_3['severity_index']}\")\n",
    "print(f\"   - S·ªë th∆∞∆°ng vong: {disaster_example_3['casualties']}\")\n",
    "print(f\"   - Thi·ªát h·∫°i kinh t·∫ø: ${disaster_example_3['economic_loss_usd']:,}\")\n",
    "print(f\"   - Th·ªùi gian ph·∫£n ·ª©ng: {disaster_example_3['response_time_hours']} gi·ªù\")\n",
    "print(f\"   üìå D·ª∞ ƒêO√ÅN S·ªê NG√ÄY H·ªíI PH·ª§C: {prediction_3} ng√†y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e83d2",
   "metadata": {},
   "source": [
    "## 10. D·ª± ƒëo√°n t√πy ch·ªânh - Nh·∫≠p d·ªØ li·ªáu c·ªßa b·∫°n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f903c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# üìù NH·∫¨P TH√îNG TIN TH·∫¢M H·ªåA M·ªöI T·∫†I ƒê√ÇY\n",
    "# ============================================\n",
    "\n",
    "my_disaster = {\n",
    "    'country': 'Brazil',            # Qu·ªëc gia\n",
    "    'disaster_type': 'Flood',       # Lo·∫°i th·∫£m h·ªça\n",
    "    'severity_index': 6.5,          # M·ª©c ƒë·ªô nghi√™m tr·ªçng (0-10)\n",
    "    'casualties': 120,              # S·ªë th∆∞∆°ng vong\n",
    "    'economic_loss_usd': 5500000,   # Thi·ªát h·∫°i kinh t·∫ø (USD)\n",
    "    'response_time_hours': 10,      # Th·ªùi gian ph·∫£n ·ª©ng (gi·ªù)\n",
    "    'aid_amount_usd': 350000,       # S·ªë ti·ªÅn vi·ªán tr·ª£ (USD)\n",
    "    'response_efficiency_score': 78, # ƒêi·ªÉm hi·ªáu qu·∫£ ph·∫£n ·ª©ng (0-100)\n",
    "    'latitude': -15.7801,           # Vƒ© ƒë·ªô\n",
    "    'longitude': -47.9292,          # Kinh ƒë·ªô\n",
    "    'year': 2024,                   # NƒÉm\n",
    "    'month': 11                     # Th√°ng\n",
    "}\n",
    "\n",
    "# Th·ª±c hi·ªán d·ª± ƒëo√°n\n",
    "my_prediction = predictor.predict(my_disaster)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üåç Qu·ªëc gia: {my_disaster['country']}\")\n",
    "print(f\"üåã Lo·∫°i th·∫£m h·ªça: {my_disaster['disaster_type']}\")\n",
    "print(f\"üìä M·ª©c ƒë·ªô nghi√™m tr·ªçng: {my_disaster['severity_index']}/10\")\n",
    "print(f\"üíÄ S·ªë th∆∞∆°ng vong: {my_disaster['casualties']:,}\")\n",
    "print(f\"üí∞ Thi·ªát h·∫°i kinh t·∫ø: ${my_disaster['economic_loss_usd']:,}\")\n",
    "print(f\"‚è±Ô∏è Th·ªùi gian ph·∫£n ·ª©ng: {my_disaster['response_time_hours']} gi·ªù\")\n",
    "print(f\"üÜò Ti·ªÅn vi·ªán tr·ª£: ${my_disaster['aid_amount_usd']:,}\")\n",
    "print(f\"üìà Hi·ªáu qu·∫£ ph·∫£n ·ª©ng: {my_disaster['response_efficiency_score']}/100\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüéØ D·ª∞ ƒêO√ÅN S·ªê NG√ÄY H·ªíI PH·ª§C: {my_prediction} NG√ÄY\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81f9bf",
   "metadata": {},
   "source": [
    "## 11. L∆∞u m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251af666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# L∆∞u m√¥ h√¨nh CatBoost\n",
    "model.save_model('../models/catboost_recovery_days.cbm')\n",
    "\n",
    "# L∆∞u feature list\n",
    "import json\n",
    "config = {\n",
    "    'feature_cols': feature_cols,\n",
    "    'categorical_features': categorical_features,\n",
    "    'train_metrics': train_metrics,\n",
    "    'test_metrics': test_metrics\n",
    "}\n",
    "\n",
    "with open('../models/catboost_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ ƒê√£ l∆∞u m√¥ h√¨nh t·∫°i:\")\n",
    "print(\"   - ../models/catboost_recovery_days.cbm\")\n",
    "print(\"   - ../models/catboost_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551744af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√°ch t·∫£i l·∫°i m√¥ h√¨nh ƒë·ªÉ s·ª≠ d·ª•ng\n",
    "print(\"üìå Code ƒë·ªÉ t·∫£i l·∫°i m√¥ h√¨nh:\")\n",
    "print(\"\"\"\n",
    "from catboost import CatBoostRegressor\n",
    "import json\n",
    "\n",
    "# T·∫£i m√¥ h√¨nh\n",
    "model = CatBoostRegressor()\n",
    "model.load_model('../models/catboost_recovery_days.cbm')\n",
    "\n",
    "# T·∫£i config\n",
    "with open('../models/catboost_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "feature_cols = config['feature_cols']\n",
    "categorical_features = config['categorical_features']\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77634719",
   "metadata": {},
   "source": [
    "## 12. T·ªïng k·∫øt\n",
    "\n",
    "### üìä K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c:\n",
    "- ƒê√£ x√¢y d·ª±ng m√¥ h√¨nh CatBoost Regressor ƒë·ªÉ d·ª± ƒëo√°n s·ªë ng√†y h·ªìi ph·ª•c sau th·∫£m h·ªça\n",
    "- Dataset: 50,000 b·∫£n ghi v·ªõi 11 features g·ªëc + 4 features m·ªõi\n",
    "- CatBoost x·ª≠ l√Ω tr·ª±c ti·∫øp categorical features (country, disaster_type)\n",
    "\n",
    "### üîß Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:\n",
    "1. **Bi·∫øn th·ªùi gian:** Tr√≠ch xu·∫•t year, month t·ª´ date\n",
    "2. **Bi·∫øn ph√¢n lo·∫°i:** CatBoost x·ª≠ l√Ω tr·ª±c ti·∫øp\n",
    "3. **Bi·∫øn s·ªë:** Log-transform cho economic_loss_usd v√† aid_amount_usd\n",
    "4. **Feature Engineering:**\n",
    "   - loss_per_casualty\n",
    "   - aid_per_hour\n",
    "   - severity_response_ratio\n",
    "   - aid_efficiency\n",
    "\n",
    "### üìà Metrics ƒë√°nh gi√°:\n",
    "- **MAE:** Sai s·ªë tuy·ªát ƒë·ªëi trung b√¨nh\n",
    "- **RMSE:** CƒÉn b·∫≠c hai sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh\n",
    "- **R¬≤:** T·ªâ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch\n",
    "\n",
    "### üéØ K·∫øt lu·∫≠n:\n",
    "CatBoost cho k·∫øt qu·∫£ t·ªët nh·ªù:\n",
    "- X·ª≠ l√Ω hi·ªáu qu·∫£ categorical features\n",
    "- Ordered boosting gi·∫£m overfitting\n",
    "- T·ªëi ∆∞u cho dataset l·ªõn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
