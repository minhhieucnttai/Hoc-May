# -*- coding: utf-8 -*-

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import pickle
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Import CatBoost
try:
    from catboost import CatBoostRegressor
    HAS_CATBOOST = True
except ImportError:
    HAS_CATBOOST = False

# Import SHAP
try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False

# =========================================================
# C·∫§U H√åNH TRANG
# =========================================================
st.set_page_config(
    page_title="Recovery Days Prediction",
    page_icon="üåç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# =========================================================
# CSS T√ôY CH·ªàNH
# =========================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-top: 10px;
        padding-bottom: 10px;
    }
</style>
""", unsafe_allow_html=True)


# =========================================================
# H√ÄM H·ªñ TR·ª¢
# =========================================================
@st.cache_data
def load_real_data():
    """Load d·ªØ li·ªáu th·ª±c t·ª´ file CSV."""
    base_path = Path(__file__).parent
    
    # T√¨m file d·ªØ li·ªáu
    data_paths = [
        base_path / "data" / "global_disaster_response_2018_2024.csv",
        base_path.parent / "data" / "global_disaster_response_2018_2024.csv",
        base_path.parent / "src" / "data" / "global_disaster_response_2018_2024.csv",
    ]
    
    for path in data_paths:
        if path.exists():
            df = pd.read_csv(path)
            return df, str(path)
    
    return None, None


def preprocess_data(df):
    """Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu."""
    df = df.copy()
    
    # Convert date
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], dayfirst=True, errors='coerce')
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        df['quarter'] = df['date'].dt.quarter
        # Drop date column to avoid Arrow serialization issues
        df = df.drop(columns=['date'])
    
    # Handle missing values
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        if df[col].isnull().any():
            df[col] = df[col].fillna(df[col].median())
    
    df = df.dropna()
    return df


def engineer_features(df):
    """T·∫°o c√°c features m·ªõi."""
    df = df.copy()
    
    # Log transforms
    if 'economic_loss_usd' in df.columns:
        df['economic_loss_log'] = np.log1p(df['economic_loss_usd'])
    if 'aid_amount_usd' in df.columns:
        df['aid_amount_log'] = np.log1p(df['aid_amount_usd'])
    if 'casualties' in df.columns:
        df['casualties_log'] = np.log1p(df['casualties'])
    
    # Ratio features
    if 'aid_amount_usd' in df.columns and 'economic_loss_usd' in df.columns:
        df['aid_coverage_ratio'] = df['aid_amount_usd'] / (df['economic_loss_usd'] + 1)
    
    if 'casualties' in df.columns and 'response_time_hours' in df.columns:
        df['casualty_per_hour'] = df['casualties'] / (df['response_time_hours'] + 1)
    
    return df


def prepare_features_for_model(df, target_col='recovery_days'):
    """Chu·∫©n b·ªã features cho model."""
    df = df.copy()
    
    # Numeric features
    numeric_features = [
        'severity_index', 'casualties', 'economic_loss_usd',
        'response_time_hours', 'aid_amount_usd', 'response_efficiency_score',
        'latitude', 'longitude'
    ]
    
    # Add time features
    if 'year' in df.columns:
        numeric_features.append('year')
    if 'month' in df.columns:
        numeric_features.append('month')
    
    # Add engineered features
    for col in ['economic_loss_log', 'aid_amount_log', 'casualties_log', 
                'aid_coverage_ratio', 'casualty_per_hour']:
        if col in df.columns:
            numeric_features.append(col)
    
    # Categorical features - encode them
    categorical_features = ['country', 'disaster_type']
    label_encoders = {}
    
    for col in categorical_features:
        if col in df.columns:
            le = LabelEncoder()
            df[col + '_encoded'] = le.fit_transform(df[col].astype(str))
            label_encoders[col] = le
            numeric_features.append(col + '_encoded')
    
    # Filter available features
    available_features = [f for f in numeric_features if f in df.columns]
    
    X = df[available_features].copy()
    y = df[target_col].copy() if target_col in df.columns else None
    
    return X, y, label_encoders, available_features


def calculate_metrics(y_true, y_pred):
    """T√≠nh c√°c metrics ƒë√°nh gi√°."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    # MAPE
    mask = y_true != 0
    if mask.sum() > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = 0.0
    
    return {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'MAPE': mape
    }


# =========================================================
# LOAD D·ªÆ LI·ªÜU
# =========================================================
df_raw, data_path = load_real_data()

if df_raw is None:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu! Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c data/")
    st.stop()

# Preprocess v√† engineer features
df = preprocess_data(df_raw)
df = engineer_features(df)


# =========================================================
# SIDEBAR
# =========================================================
st.sidebar.title("üåç Recovery Days Prediction")
st.sidebar.markdown("---")

st.sidebar.success(f"‚úÖ ƒê√£ load d·ªØ li·ªáu th·ª±c")
st.sidebar.caption(f"üìÅ {Path(data_path).name}")

st.sidebar.markdown("---")
st.sidebar.markdown("""
### üìä Th√¥ng tin Dataset
""")
st.sidebar.write(f"- S·ªë b·∫£n ghi: **{len(df):,}**")
st.sidebar.write(f"- S·ªë features: **{len(df.columns)}**")
st.sidebar.write(f"- Recovery Days: **{df['recovery_days'].min():.0f} - {df['recovery_days'].max():.0f}**")

st.sidebar.markdown("---")
st.sidebar.markdown("""
### üè∑Ô∏è Disaster Types
""")
for dtype in df['disaster_type'].unique()[:5]:
    count = len(df[df['disaster_type'] == dtype])
    st.sidebar.write(f"- {dtype}: {count:,}")


# =========================================================
# HEADER CH√çNH
# =========================================================
st.markdown('<h1 class="main-header">üåç D·ª± ƒêo√°n S·ªë Ng√†y Ph·ª•c H·ªìi Sau Th·∫£m H·ªça</h1>', unsafe_allow_html=True)
st.markdown("""
<p style="text-align: center; font-size: 1.1rem; color: #666;">
    Machine Learning Project - S·ª≠ d·ª•ng CatBoost Regressor ƒë·ªÉ d·ª± ƒëo√°n recovery_days<br>
    <b>Dataset: Global Disaster Response 2018-2024</b>
</p>
""", unsafe_allow_html=True)


# =========================================================
# TABS CH√çNH
# =========================================================
tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "üìä T·ªïng quan d·ªØ li·ªáu",
    "üìà Ph√¢n t√≠ch EDA", 
    "ü§ñ Hu·∫•n luy·ªán m√¥ h√¨nh",
    "üéØ D·ª± ƒëo√°n",
    "üìã V·ªÅ Project"
])


# =========================================================
# TAB 1: T·ªîNG QUAN D·ªÆ LI·ªÜU
# =========================================================
with tab1:
    st.header("üìä T·ªïng quan d·ªØ li·ªáu")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("S·ªë b·∫£n ghi", f"{len(df):,}")
    with col2:
        st.metric("S·ªë features", len(df.columns))
    with col3:
        st.metric("Recovery Days (Mean)", f"{df['recovery_days'].mean():.1f}")
    with col4:
        st.metric("Recovery Days (Median)", f"{df['recovery_days'].median():.1f}")
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìã M·∫´u d·ªØ li·ªáu")
        st.dataframe(df.head(10), use_container_width=True)
    
    with col2:
        st.subheader("üìà Th·ªëng k√™ m√¥ t·∫£")
        st.dataframe(df.describe(), use_container_width=True)
    
    st.markdown("---")
    st.subheader("üîç Ki·ªÉu d·ªØ li·ªáu c√°c c·ªôt")
    dtype_df = pd.DataFrame({
        'Column': df.columns,
        'Type': [str(t) for t in df.dtypes.values],
        'Non-Null': df.notnull().sum().values,
        'Missing': df.isnull().sum().values
    })
    st.dataframe(dtype_df, use_container_width=True)


# =========================================================
# TAB 2: PH√ÇN T√çCH EDA
# =========================================================
with tab2:
    st.header("üìà Ph√¢n t√≠ch kh√°m ph√° d·ªØ li·ªáu (EDA)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Ph√¢n b·ªë Recovery Days")
        fig = px.histogram(df, x='recovery_days', nbins=50, 
                          color_discrete_sequence=['steelblue'])
        fig.update_layout(
            xaxis_title="S·ªë ng√†y ph·ª•c h·ªìi",
            yaxis_title="T·∫ßn su·∫•t"
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Boxplot Recovery Days")
        fig = px.box(df, y='recovery_days', color_discrete_sequence=['steelblue'])
        fig.update_layout(yaxis_title="S·ªë ng√†y ph·ª•c h·ªìi")
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Recovery Days theo Disaster Type")
        fig = px.box(df, x='disaster_type', y='recovery_days', 
                    color='disaster_type')
        fig.update_layout(xaxis_tickangle=-45, showlegend=False)
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Recovery Days theo Country (Top 15)")
        country_mean = df.groupby('country')['recovery_days'].mean().sort_values(ascending=False).head(15)
        fig = px.bar(x=country_mean.index, y=country_mean.values,
                    labels={'x': 'Country', 'y': 'Mean Recovery Days'})
        fig.update_layout(xaxis_tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    st.subheader("Ma tr·∫≠n t∆∞∆°ng quan")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    # Ch·ªçn c√°c c·ªôt ch√≠nh ƒë·ªÉ hi·ªÉn th·ªã
    main_cols = ['recovery_days', 'severity_index', 'casualties', 'economic_loss_usd',
                 'response_time_hours', 'aid_amount_usd', 'response_efficiency_score']
    main_cols = [c for c in main_cols if c in numeric_cols]
    corr_matrix = df[main_cols].corr()
    fig = px.imshow(corr_matrix, text_auto='.2f', aspect='auto',
                   color_continuous_scale='RdBu_r')
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Scatter: Severity vs Recovery Days")
        fig = px.scatter(df.sample(min(2000, len(df)), random_state=42), 
                        x='severity_index', y='recovery_days',
                        opacity=0.5, color='disaster_type')
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Scatter: Economic Loss vs Recovery Days")
        df_sample = df.sample(min(2000, len(df)), random_state=42)
        fig = px.scatter(df_sample, 
                        x=np.log1p(df_sample['economic_loss_usd']), 
                        y='recovery_days',
                        opacity=0.5, color='disaster_type',
                        labels={'x': 'Log(Economic Loss USD)'})
        st.plotly_chart(fig, use_container_width=True)


# =========================================================
# TAB 3: HU·∫§N LUY·ªÜN M√î H√åNH
# =========================================================
with tab3:
    st.header("ü§ñ Hu·∫•n luy·ªán m√¥ h√¨nh CatBoost")
    
    if not HAS_CATBOOST:
        st.error("‚ùå CatBoost ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. Vui l√≤ng ch·∫°y: pip install catboost")
        st.stop()
    
    st.markdown("""
    ### M√¥ h√¨nh: CatBoost Regressor
    
    **L√Ω do ch·ªçn CatBoost:**
    - ‚úÖ X·ª≠ l√Ω t·ªët bi·∫øn ph√¢n lo·∫°i (country, disaster_type)
    - ‚úÖ Kh√¥ng c·∫ßn One-Hot Encoding
    - ‚úÖ Hi·ªáu su·∫•t cao v·ªõi dataset v·ª´a-l·ªõn
    - ‚úÖ √çt overfitting v·ªõi Ordered Boosting
    """)
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    
    with col1:
        test_size = st.slider("Test size (%)", 10, 40, 20) / 100
        iterations = st.slider("S·ªë iterations", 100, 1000, 300)
        learning_rate = st.select_slider("Learning rate", 
                                        options=[0.01, 0.03, 0.05, 0.1, 0.2],
                                        value=0.1)
    
    with col2:
        depth = st.slider("Depth", 4, 12, 6)
        l2_leaf_reg = st.slider("L2 regularization", 1, 10, 3)
    
    if st.button("üöÄ Hu·∫•n luy·ªán m√¥ h√¨nh", type="primary"):
        with st.spinner("ƒêang hu·∫•n luy·ªán m√¥ h√¨nh..."):
            # Chu·∫©n b·ªã d·ªØ li·ªáu
            X, y, encoders, feature_names = prepare_features_for_model(df, 'recovery_days')
            
            # Chia d·ªØ li·ªáu
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=test_size, random_state=42
            )
            
            # X√°c ƒë·ªãnh categorical features
            cat_features = [i for i, col in enumerate(X.columns) if '_encoded' in col]
            
            # Train model
            model = CatBoostRegressor(
                iterations=iterations,
                learning_rate=learning_rate,
                depth=depth,
                l2_leaf_reg=l2_leaf_reg,
                random_state=42,
                verbose=False
            )
            
            model.fit(X_train, y_train, cat_features=cat_features)
            
            # D·ª± ƒëo√°n v√† ƒë√°nh gi√°
            y_pred = model.predict(X_test)
            metrics = calculate_metrics(y_test.values, y_pred)
            
            # L∆∞u v√†o session state
            st.session_state['model'] = model
            st.session_state['X'] = X
            st.session_state['feature_names'] = feature_names
            st.session_state['encoders'] = encoders
            st.session_state['cat_features'] = cat_features
            
            st.success("‚úÖ ƒê√£ hu·∫•n luy·ªán xong m√¥ h√¨nh!")
            
            # Hi·ªÉn th·ªã k·∫øt qu·∫£
            st.markdown("### üìä K·∫øt qu·∫£ ƒë√°nh gi√°")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("MAE", f"{metrics['MAE']:.2f} ng√†y")
            with col2:
                st.metric("RMSE", f"{metrics['RMSE']:.2f} ng√†y")
            with col3:
                st.metric("R¬≤ Score", f"{metrics['R2']:.4f}")
            with col4:
                st.metric("MAPE", f"{metrics['MAPE']:.2f}%")
            
            st.markdown("---")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Actual vs Predicted")
                fig = px.scatter(x=y_test, y=y_pred, opacity=0.3,
                               labels={'x': 'Actual', 'y': 'Predicted'})
                fig.add_trace(go.Scatter(
                    x=[y_test.min(), y_test.max()],
                    y=[y_test.min(), y_test.max()],
                    mode='lines',
                    name='Perfect Prediction',
                    line=dict(color='red', dash='dash')
                ))
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.subheader("Feature Importance")
                importances = model.get_feature_importance()
                importance_df = pd.DataFrame({
                    'feature': X.columns,
                    'importance': importances
                }).sort_values('importance', ascending=False)
                
                fig = px.bar(importance_df.head(15), x='importance', y='feature',
                           orientation='h')
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
            
            # SHAP Explainability
            if HAS_SHAP:
                st.markdown("---")
                st.subheader("üîç SHAP Explainability")
                
                with st.spinner("ƒêang t√≠nh SHAP values..."):
                    sample_size = min(500, len(X_test))
                    X_sample = X_test.sample(n=sample_size, random_state=42)
                    
                    explainer = shap.Explainer(model)
                    shap_values = explainer(X_sample)
                    
                    fig, ax = plt.subplots(figsize=(10, 6))
                    shap.summary_plot(shap_values, X_sample, show=False)
                    st.pyplot(fig)
                    plt.close()


# =========================================================
# TAB 4: D·ª∞ ƒêO√ÅN
# =========================================================
with tab4:
    st.header("üéØ D·ª± ƒëo√°n s·ªë ng√†y ph·ª•c h·ªìi")
    
    st.markdown("""
    Nh·∫≠p th√¥ng tin v·ªÅ th·∫£m h·ªça ƒë·ªÉ d·ª± ƒëo√°n s·ªë ng√†y ph·ª•c h·ªìi:
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        country = st.selectbox("Qu·ªëc gia", sorted(df['country'].unique()))
        disaster_type = st.selectbox("Lo·∫°i th·∫£m h·ªça", sorted(df['disaster_type'].unique()))
        severity_index = st.slider("Ch·ªâ s·ªë nghi√™m tr·ªçng (1-10)", 1.0, 10.0, 5.0)
        casualties = st.number_input("S·ªë th∆∞∆°ng vong", 0, 100000, 100)
        economic_loss = st.number_input("Thi·ªát h·∫°i kinh t·∫ø (USD)", 0, 1000000000, 1000000)
    
    with col2:
        response_time = st.slider("Th·ªùi gian ph·∫£n ·ª©ng (gi·ªù)", 1.0, 500.0, 24.0)
        aid_amount = st.number_input("S·ªë ti·ªÅn vi·ªán tr·ª£ (USD)", 0, 500000000, 500000)
        efficiency_score = st.slider("ƒêi·ªÉm hi·ªáu qu·∫£ ph·∫£n ·ª©ng (0-1)", 0.0, 1.0, 0.5)
        latitude = st.slider("Vƒ© ƒë·ªô", -90.0, 90.0, 0.0)
        longitude = st.slider("Kinh ƒë·ªô", -180.0, 180.0, 0.0)
    
    if st.button("üîÆ D·ª± ƒëo√°n", type="primary"):
        if 'model' not in st.session_state:
            st.warning("‚ö†Ô∏è Vui l√≤ng hu·∫•n luy·ªán m√¥ h√¨nh tr∆∞·ªõc (Tab 'Hu·∫•n luy·ªán m√¥ h√¨nh')")
        else:
            model = st.session_state['model']
            encoders = st.session_state['encoders']
            X_template = st.session_state['X']
            
            # Encode categorical features
            country_encoded = encoders['country'].transform([country])[0] if country in encoders['country'].classes_ else 0
            disaster_encoded = encoders['disaster_type'].transform([disaster_type])[0] if disaster_type in encoders['disaster_type'].classes_ else 0
            
            # T·∫°o input data
            input_data = pd.DataFrame({col: [0] for col in X_template.columns})
            
            # Fill values
            input_data['severity_index'] = severity_index
            input_data['casualties'] = casualties
            input_data['economic_loss_usd'] = economic_loss
            input_data['response_time_hours'] = response_time
            input_data['aid_amount_usd'] = aid_amount
            input_data['response_efficiency_score'] = efficiency_score
            input_data['latitude'] = latitude
            input_data['longitude'] = longitude
            
            if 'year' in input_data.columns:
                input_data['year'] = 2024
            if 'month' in input_data.columns:
                input_data['month'] = 6
            
            if 'country_encoded' in input_data.columns:
                input_data['country_encoded'] = country_encoded
            if 'disaster_type_encoded' in input_data.columns:
                input_data['disaster_type_encoded'] = disaster_encoded
            
            # Engineered features
            if 'economic_loss_log' in input_data.columns:
                input_data['economic_loss_log'] = np.log1p(economic_loss)
            if 'aid_amount_log' in input_data.columns:
                input_data['aid_amount_log'] = np.log1p(aid_amount)
            if 'casualties_log' in input_data.columns:
                input_data['casualties_log'] = np.log1p(casualties)
            if 'aid_coverage_ratio' in input_data.columns:
                input_data['aid_coverage_ratio'] = aid_amount / (economic_loss + 1)
            if 'casualty_per_hour' in input_data.columns:
                input_data['casualty_per_hour'] = casualties / (response_time + 1)
            
            # D·ª± ƒëo√°n
            prediction = model.predict(input_data)[0]
            
            st.markdown("---")
            st.markdown(f"""
            <div style="text-align: center; padding: 2rem; background-color: #e8f4f8; border-radius: 1rem;">
                <h2 style="color: #1E88E5;">K·∫øt qu·∫£ d·ª± ƒëo√°n</h2>
                <h1 style="font-size: 4rem; color: #0D47A1;">{prediction:.1f}</h1>
                <h3>ng√†y ph·ª•c h·ªìi</h3>
            </div>
            """, unsafe_allow_html=True)
            
            # Ph√¢n t√≠ch
            st.markdown("---")
            st.subheader("üìù Ph√¢n t√≠ch")
            
            if prediction < 30:
                st.success("üü¢ D·ª± ki·∫øn ph·ª•c h·ªìi NHANH (< 1 th√°ng)")
            elif prediction < 90:
                st.warning("üü° D·ª± ki·∫øn ph·ª•c h·ªìi TRUNG B√åNH (1-3 th√°ng)")
            else:
                st.error("üî¥ D·ª± ki·∫øn ph·ª•c h·ªìi CH·∫¨M (> 3 th√°ng)")
            
            # So s√°nh v·ªõi d·ªØ li·ªáu
            st.markdown("---")
            st.subheader("üìä So s√°nh v·ªõi d·ªØ li·ªáu th·ª±c")
            
            similar = df[df['disaster_type'] == disaster_type]
            if len(similar) > 0:
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(f"Mean ({disaster_type})", f"{similar['recovery_days'].mean():.1f} ng√†y")
                with col2:
                    st.metric(f"Min ({disaster_type})", f"{similar['recovery_days'].min():.1f} ng√†y")
                with col3:
                    st.metric(f"Max ({disaster_type})", f"{similar['recovery_days'].max():.1f} ng√†y")


# =========================================================
# TAB 5: V·ªÄ PROJECT
# =========================================================
with tab5:
    st.header("üìã V·ªÅ Project")
    
    st.markdown("""
    ## D·ª± ƒëo√°n s·ªë ng√†y ph·ª•c h·ªìi sau th·∫£m h·ªça to√†n c·∫ßu
    
    ### 1. Gi·ªõi thi·ªáu
    
    Project n√†y x√¢y d·ª±ng m√¥ h√¨nh Machine Learning ƒë·ªÉ d·ª± ƒëo√°n **s·ªë ng√†y ph·ª•c h·ªìi (recovery_days)** 
    sau c√°c th·∫£m h·ªça t·ª± nhi√™n tr√™n to√†n c·∫ßu. ƒê√¢y l√† b√†i to√°n **h·ªìi quy (Regression)**.
    
    ### 2. Dataset
    
    - **Ngu·ªìn**: Global Disaster Response 2018-2024
    - **Quy m√¥**: ~50,000 b·∫£n ghi
    - **Bi·∫øn m·ª•c ti√™u**: recovery_days
    - **Features ch√≠nh**: severity_index, casualties, economic_loss_usd, response_time_hours, aid_amount_usd
    
    ### 3. M√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn: CatBoost Regressor
    
    **L√Ω do ch·ªçn:**
    - ‚úÖ X·ª≠ l√Ω t·ªët bi·∫øn ph√¢n lo·∫°i (country, disaster_type) - kh√¥ng c·∫ßn One-Hot Encoding
    - ‚úÖ B·∫Øt ƒë∆∞·ª£c quan h·ªá phi tuy·∫øn gi·ªØa c√°c bi·∫øn
    - ‚úÖ Hi·ªáu su·∫•t cao v·ªõi dataset v·ª´a-l·ªõn (50k d√≤ng)
    - ‚úÖ √çt overfitting nh·ªù Ordered Boosting
    - ‚úÖ H·ªó tr·ª£ gi·∫£i th√≠ch m√¥ h√¨nh (Feature Importance, SHAP)
    
    ### 4. Pipeline
    
    ```
    Data Loading ‚Üí Preprocessing ‚Üí Feature Engineering ‚Üí Training ‚Üí Evaluation ‚Üí Prediction
    ```
    
    ### 5. ƒê√°nh gi√° m√¥ h√¨nh
    
    S·ª≠ d·ª•ng c√°c ch·ªâ s·ªë **h·ªìi quy**:
    - **MAE** (Mean Absolute Error): Sai s·ªë trung b√¨nh tuy·ªát ƒë·ªëi
    - **RMSE** (Root Mean Squared Error): CƒÉn b·∫≠c hai sai s·ªë b√¨nh ph∆∞∆°ng trung b√¨nh
    - **R¬≤ Score**: H·ªá s·ªë x√°c ƒë·ªãnh
    - **MAPE** (Mean Absolute Percentage Error): Ph·∫ßn trƒÉm sai s·ªë trung b√¨nh
    
    ### 6. K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c
    
    - **R¬≤ Score**: ~0.94 (94% variance ƒë∆∞·ª£c gi·∫£i th√≠ch)
    - **MAE**: ~4 ng√†y
    - **RMSE**: ~5 ng√†y
    
    ### 7. T√°c gi·∫£
    
    **Tr·∫ßn Minh Hi·∫øu**
    
    ---
    
    ### üìö T√†i li·ªáu tham kh·∫£o
    
    1. Prokhorenkova et al., *CatBoost: unbiased boosting with categorical features*, NeurIPS, 2018
    2. Lundberg & Lee, *A Unified Approach to Interpreting Model Predictions*, NeurIPS, 2017
    3. EM-DAT: The International Disaster Database
    4. World Bank Open Data
    """)


# =========================================================
# FOOTER
# =========================================================
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #888; padding: 1rem;">
    <p>üìö Machine Learning Project - Recovery Days Prediction</p>
    <p>T√°c gi·∫£: Tr·∫ßn Minh Hi·∫øu | ¬© 2026</p>
</div>
""", unsafe_allow_html=True)
